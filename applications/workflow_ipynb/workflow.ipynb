{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Overview\n",
    " The **objective** of this notebook is to present the workflow of conducting data assimilation on [PFLOTRAN](https://www.pflotran.org/) by using [DART](https://www.image.ucar.edu/DAReS/DART/). Briefly, the procedures are as follows:\n",
    " - [x] [Configuration](#parameter): define directories, file locations, and other parameters\n",
    " - [x] [PFLOTRAN preparation](#pflotran_prepare): generate PFLOTRAN input files\n",
    " - [x] [PFLOTRAN model spin-up](#pflotran_spinup): conduct model spin-up\n",
    " - [x] [DART files preparation](#dart_prepare): add new DART quantities, prepare DART input namelists, prepare DART prior data, prepare observations in DART format, and check ```model_mod``` interface\n",
    " - [x] [Generate all the executable files](#dart_executables): generate all the executables, convert observations in DART format, check ```model_mod``` interface, and test the filter\n",
    " - [x] [Run DART and PFLOTRAN](#run_dart_pflotran): run the shell script for integrating DART filter and PFLOTRAN model\n",
    "\n",
    " Here, we perform inverse modeling on a 1D thermal model for illustration. The model assimilates temperature observation to update its parameters (i.e., flow flux, porosity, and thermal conductivity). For now, the ensemble Kalman filter (EnKF) is used for assimilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='parameter'></a>\n",
    " # Step 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import f90nml\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from math import floor, ceil\n",
    "from datetime import datetime, timedelta\n",
    "# get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "# get_ipython().run_line_magic('autoreload', '2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Define the locations of MPI, PFLOTRAN, application folder and DART-PFLOTRAN interface folder**\n",
    "\n",
    " It is suggested that <span style=\"background-color:lightgreen\">mpi_exe</span> is defined based on the mpi utility (e.g., mpirun) installed by PFLOTRAN.\n",
    "\n",
    " **Important:** You must make sure the <span style=\"background-color:lightgreen\">mpi_exe</span> has the same MPI system as the settings ```MPIFC``` and ```MPILD``` in ```mkmf.template```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPI settings\n",
    "mpi_exe_da  = '/usr/local/bin/mpirun'  # The location of mpirun\n",
    "mpi_exe_pf  = '/Users/jian449/Codes/petsc/arch-darwin-c-opt/bin/mpirun'\n",
    "ncore_da = 1  # The number of MPI cores used by DART\n",
    "ncore_pf = 1  # The number of MPI cores used by PFLOTRAN\n",
    "ngroup_pf= 1  # The number of group used by stochastic running in PFLOTRAN\n",
    "\n",
    "# PFLOTRAN executable\n",
    "# pflotran_exe  = '/global/project/projectdirs/m1800/pin/pflotran-haswell/src/pflotran/pflotran'\n",
    "pflotran_exe  = '/Users/jian449/Codes/pflotran/src/pflotran/pflotran'\n",
    "\n",
    "# Main directory names\n",
    "temp_app_dir = \"/Users/jian449/Codes/DART/manhattan/models/pflotran/applications/template\"          # The template for application folder\n",
    "app_dir      = \"/Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min\"          # The application folder name\n",
    "dart_dir     = \"/Users/jian449/Codes/DART/manhattan\"\n",
    "dart_pf_dir  = \"/Users/jian449/Codes/DART/manhattan/models/pflotran\"     # The dart pflotran utitlity folder name\n",
    "# temp_app_dir = \"/global/cscratch1/sd/peishi89/DART_PFLOTRAN_APP/applications/template\"          # The template for application folder\n",
    "# app_dir      = \"/global/cscratch1/sd/peishi89/DART_PFLOTRAN_APP/applications/1dthermal\"          # The application folder name\n",
    "# dart_dir     = \"/global/homes/p/peishi89/DART/manhattan\"\n",
    "# dart_pf_dir  = \"/global/homes/p/peishi89/DART/manhattan/models/pflotran\"     # The dart pflotran utitlity folder name\n",
    "#temp_app_dir = os.path.abspath(\"../template\" )          # The template for application folder\n",
    "#app_dir      = os.path.abspath(\"../1dthermal/\")          # The application folder name\n",
    "#dart_dir     = os.path.abspath(\"../../../../\")\n",
    "#dart_pf_dir  = os.path.join(dart_dir, \"models/pflotran\")     # The dart pflotran utitlity folder name\n",
    "\n",
    "# configs = {}\n",
    "configs = f90nml.namelist.Namelist()\n",
    "configs[\"main_dir_cfg\"] = {\"app_dir\": app_dir, \"dart_dir\": dart_dir, \"dart_pf_dir\": dart_pf_dir}\n",
    "configs[\"exe_cfg\"]      = {\"pflotran_exe\": pflotran_exe, \"mpi_exe_da\": mpi_exe_da, \"mpi_exe_pf\": mpi_exe_pf, \n",
    "                           \"ncore_pf\": ncore_pf, \"ncore_da\": ncore_da, \"ngroup_pf\": ngroup_pf}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Generate the application directory if it does not exit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the application directory if it does not exists\n",
    "if not os.path.isdir(app_dir):\n",
    "    shutil.copytree(temp_app_dir, app_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Load all the required file paths/names from ```file_paths.nml```**\n",
    "\n",
    " ```file_paths.nml``` defines the relative locations of all the files (e.g., utility files, shell scripts, data files, etc) used by this application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(dart_pf_dir)\n",
    "from utils.read_filepaths_nml import read_filepaths_nml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs_cfg, files_cfg      = read_filepaths_nml(app_dir=app_dir, dart_pf_dir=dart_pf_dir)\n",
    "configs[\"other_dir_cfg\"] = dirs_cfg\n",
    "configs[\"file_cfg\" ]     = files_cfg\n",
    "config_file              = files_cfg[\"config_file\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Clean up the folders under the app_dir**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='cd /Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min/dart_inout; ls | xargs rm', returncode=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pflotran_in_dir   = dirs_cfg[\"pflotran_in_dir\"]\n",
    "pflotran_out_dir  = dirs_cfg[\"pflotran_out_dir\"]\n",
    "dart_inout_dir    = dirs_cfg[\"dart_data_dir\"]\n",
    "# subprocess.run(\"cd {}; rm *\".format(pflotran_in_dir), shell=True, check=False)\n",
    "subprocess.run(\"cd {}; rm -f *\".format(pflotran_out_dir), shell=True, check=True)\n",
    "subprocess.run(\"cd {}; ls | xargs rm\".format(dart_inout_dir), shell=True, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Specify the following types of variables used in DART**\n",
    " - the observation data to be assimilated\n",
    " - the PFLOTRAN parameters to be analyzed\n",
    " - the statistics of the parameters: (1) the value range; (2) the mean and std for randomly sampling; (3) the distribution to be sampled (i.e., normal, truncated normal, and uniform distributions).\n",
    " - The list of parameters whose prior would be resampled based on the mean of the corresponding posterior at the previous time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation data to be assimilated\n",
    "obs_set  = ['TEMPERATURE']\n",
    "\n",
    "# The PFLOTRAN parameters to be analyzed\n",
    "# para_set = ['FLOW_FLUX','POROSITY','THERMAL_CONDUCTIVITY']\n",
    "para_set = ['FLOW_FLUX']\n",
    "\n",
    "# The statistics of the parameters\n",
    "# The index order follows para_set\n",
    "# para_min_set  = [-10.0, 0.01, 0.5]  # The minimum values (-99999 means no lower bound limit)\n",
    "# para_max_set  = [10.0, 0.7, 2.5]  # The maximum values (99999 means no upper bound limit)\n",
    "# para_mean_set = [0.0, 0.3, 1.5]  # The mean values\n",
    "# para_std_set  = [0.5, 0.1, 0.5]  # The standard deviation values\n",
    "# para_dist_set = [\"normal\", \"normal\", \"normal\"]  # The assumed distribution to be sampled\n",
    "para_min_set  = [-5.0]  # The minimum values (-99999 means no lower bound limit)\n",
    "para_max_set  = [5.0]  # The maximum values (99999 means no upper bound limit)\n",
    "para_mean_set = [0.0]  # The mean values\n",
    "para_std_set  = [0.5]  # The standard deviation values\n",
    "# para_dist_set = [\"normal\"]  # The assumed distribution to be sampled\n",
    "para_dist_set = [\"test\"]  # The assumed distribution to be sampled\n",
    "\n",
    "para_resampled_set = ['FLOW_FLUX']   # The parameters to be resampled at each time step\n",
    "# para_resampled_set = ['']   # The parameters to be resampled at each time step\n",
    "\n",
    "configs[\"obspara_set_cfg\"] = {\"obs_set\": obs_set, \"para_set\": para_set,\n",
    "                              \"para_min_set\": para_min_set, \"para_max_set\": para_max_set,\n",
    "                              \"para_mean_set\": para_mean_set, \"para_std_set\": para_std_set,\n",
    "                              \"para_dist_set\": para_dist_set, \"para_resampled_set\": para_resampled_set}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Specify the spatial domains of the observation data to be assimilated**\n",
    "\n",
    " The limits of x, y, and z are bounded by the minimum and maximum boundaries through (min, max). If the limit is not specified, -99999 and 99999 are assigned for the lower and upper bounds, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space_xlimit = [-99999, 99999]\n",
    "obs_space_ylimit = [-99999, 99999]\n",
    "obs_space_zlimit = [-0.5, -0.04]\n",
    "configs[\"obs_space_cfg\"] = {\"obs_space_xlimit\": obs_space_xlimit, \"obs_space_ylimit\": obs_space_ylimit,\n",
    "                            \"obs_space_zlimit\": obs_space_zlimit}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Specify the data assimilation time window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_cfg = {}\n",
    "\n",
    "# Assimilation time window time_step_days+time_step_seconds\n",
    "# Assimilation window\n",
    "da_cfg[\"assim_window_days\"]    = 0     # assimilation time window/step (day)\n",
    "da_cfg[\"assim_window_seconds\"] = 300  # assimilation time window/step  (second)\n",
    "da_cfg[\"assim_window_size\"] = da_cfg[\"assim_window_days\"]+float(da_cfg[\"assim_window_seconds\"])/86400. # day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Specify the temporal information**\n",
    " - model spinup time/start time\n",
    " - the map between the begin of observation assimilation and model start time\n",
    " - the list of model time or the list of starting assimilation time (starting from zero)\n",
    "\n",
    " **note that** model start time is considered after the spinup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cfg = {}\n",
    "one_sec = 1./86400. # fraction of day\n",
    "# Model spinup length\n",
    "time_cfg[\"spinup_length\"]  = 1./12.    # spinup time (day)\n",
    "time_cfg[\"is_spinup_done\"] = False  # whether spinup is conducted\n",
    "\n",
    "# Model start time\n",
    "time_cfg[\"current_model_time\"] = (da_cfg[\"assim_window_size\"] + one_sec)/2  # model start time should be the middle of the first assimilation window (after spinup)\n",
    "time_cfg[\"model_time_list\"]    = [0.]   # the list of model time\n",
    "\n",
    "# Map between assimilation start time and model start time\n",
    "obs_start   = datetime(2017,4,1,0,0,0) \n",
    "assim_start = obs_start + timedelta(days=time_cfg[\"spinup_length\"]) # assimilation time should be after the model spinup\n",
    "time_cfg[\"assim_start\"] = assim_start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# The maximum time for observation\n",
    "time_cfg[\"first_obs_time_days\"]    = 0\n",
    "time_cfg[\"first_obs_time_seconds\"] = 0\n",
    "time_cfg[\"first_obs_time_size\"] = time_cfg[\"first_obs_time_days\"]+float(time_cfg[\"first_obs_time_seconds\"])/86400. # day\n",
    "time_cfg[\"last_obs_time_days\"]    = 0\n",
    "time_cfg[\"last_obs_time_seconds\"] = 300\n",
    "# time_cfg[\"last_obs_time_seconds\"] = 300*12*50\n",
    "time_cfg[\"last_obs_time_size\"] = time_cfg[\"last_obs_time_days\"]+float(time_cfg[\"last_obs_time_seconds\"])/86400. # day\n",
    "\n",
    "# Whether the model time exceeds the last observation\n",
    "time_cfg[\"exceeds_obs_time\"] = time_cfg[\"current_model_time\"] >= time_cfg[\"last_obs_time_size\"]\n",
    "\n",
    "# Save them to configs\n",
    "configs[\"time_cfg\"] = time_cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Config the data assimilation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation error, number of ensembles\n",
    "da_cfg[\"obs_reso\"]  = 300.0      # observation resolution (second)\n",
    "da_cfg[\"nens\"]      = 100         # number of ensembles\n",
    "da_cfg[\"obs_error\"] = 0.02       # the observation error\n",
    "da_cfg[\"obs_error_type\"] = \"relative\" # the type of observation error (i.e., relative and absolute)\n",
    "\n",
    "# Assimilation start and end time\n",
    "da_cfg[\"assim_start_days\"]    = int(floor(time_cfg[\"current_model_time\"] - da_cfg[\"assim_window_size\"]/2))\n",
    "da_cfg[\"assim_start_seconds\"] = int((time_cfg[\"current_model_time\"] - da_cfg[\"assim_window_size\"]/2 - da_cfg[\"assim_start_days\"])*86400+1)\n",
    "da_cfg[\"assim_end_days\"]      = int(floor(time_cfg[\"current_model_time\"] + da_cfg[\"assim_window_size\"]/2))\n",
    "da_cfg[\"assim_end_seconds\"]   = int((time_cfg[\"current_model_time\"] + da_cfg[\"assim_window_size\"]/2 - da_cfg[\"assim_end_days\"])*86400)\n",
    "\n",
    "# Compute the number of time steps\n",
    "# print(np.ceil((time_cfg[\"last_obs_time_size\"] - time_cfg[\"first_obs_time_size\"]) / da_cfg[\"assim_window_size\"]))\n",
    "da_cfg[\"ntimestep\"] = ceil((time_cfg[\"last_obs_time_size\"] - time_cfg[\"first_obs_time_size\"]) / da_cfg[\"assim_window_size\"])\n",
    "\n",
    "# Decide whether the observation posterior should be recomputed from the model output based on the parameter posterior\n",
    "da_cfg[\"obs_ens_posterior_from_model\"] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Config the MDA settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inflation settings used in EnKS-MDA (the alpha value)\n",
    "# da_cfg[\"enks_mda_alpha\"] = [4., 4., 4., 4.]  # Note that the summation of the inverse of alpha should be one\n",
    "da_cfg[\"enks_mda_alpha\"] = [1.]  # Note that the summation of the inverse of alpha should be one\n",
    "da_cfg[\"enks_mda_iteration_step\"] = 1  # the ith iteration (1 for the first iteration)\n",
    "da_cfg[\"enks_mda_total_iterations\"] = len(da_cfg[\"enks_mda_alpha\"])  # Note that the summation of the inverse of alpha should be one\n",
    "\n",
    "# Check whether the sum of the inverse of enks_mda_alpha is one\n",
    "alpha_inv_sum = sum([1./alpha for alpha in da_cfg[\"enks_mda_alpha\"]])\n",
    "if alpha_inv_sum - 1 > 1e-8:\n",
    "    raise Exception(\"The sum of the inverse of enks_mda_alpha should be one!\")\n",
    "\n",
    "# Save them to configs\n",
    "configs[\"da_cfg\"] = da_cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Save all the configurations in pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.write(config_file, force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Clean up the work, PFLOTRAN output, and DART in/out directories**\n",
    " subprocess.run(\"rm -f {}/*\".format(dirs_cfg[\"dart_data_dir\"]), shell=True, check=True)\n",
    " subprocess.run(\"rm -f {}/*\".format(dirs_cfg[\"pflotran_out_dir\"]), shell=True, check=True)\n",
    " subprocess.run(\"rm -f {}/*\".format(dirs_cfg[\"app_work_dir\"]), shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it in a temperory pickle file or namelist???\n",
    "configs.write(config_file, force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='pflotran_prepare'></a>\n",
    " # Step 2: PFLOTRAN preparation\n",
    " *Here, we use Kewei's 1D thermal model as an example for generating PFLOTRAN input card and parameter.h5.*\n",
    "\n",
    " In this section, the following procedures are performed:\n",
    " - generate PFLOTRAN input deck file ```PFLOTRAN.in```\n",
    " - generate the parameter files in HDF 5, ```parameter_prior.h5```, used by PFLOTRAN input deck file\n",
    "\n",
    " **Note that**\n",
    " - ```PFLOTRAN.in``` for each DA scenario should be prepared by users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_pflotran_inputdeck, pflotran_in = files_cfg[\"prep_pflotran_inputdeck_file\"], files_cfg[\"pflotran_in_file\"]\n",
    "prep_pflotran_parameterprior = files_cfg[\"prep_pflotran_para_file\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Generate the ensembles of PFLOTRAN prior**\n",
    "\n",
    " **Run code**\n",
    " - Run: ```prepare_pflotran_parameterprior.py```\n",
    " - Code input arguments (loaded from the configuration file):\n",
    "     - <span style=\"background-color:lightgreen\">pflotran_para</span>: filename for ```parameter_prior.h5```\n",
    "     - <span style=\"background-color:lightgreen\">obs_resolution, obs_error, nens, spinup_length, spinup</span>: data assimilation settings (i.e., observation timestep, observation error, number of ensemble, whether it is spinup, **to be revised**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prepare PFLOTRAN ensemble parameter values...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python /Users/jian449/Codes/DART/manhattan/models/pflotran/utils/prepare_pflotran_parameterprior.py /Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min/work/config.nml', returncode=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$prep_pflotran_parameterprior\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Prepare PFLOTRAN ensemble parameter values...\")\n",
    "subprocess.run(\"python {} {}\".format(prep_pflotran_parameterprior, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Generate PFLOTRAN input deck file**\n",
    "\n",
    " **Run code**\n",
    " - Run: ```prepare_pflotran_inputdeck.py```\n",
    " - Code input arguments (loaded from the configuration file):\n",
    "     - <span style=\"background-color:lightgreen\">pflotran_in</span>: filename for ```pflotran.in```\n",
    "     - <span style=\"background-color:lightgreen\">pflotran_para</span>: filename for ```parameter_prior.h5```\n",
    "     - <span style=\"background-color:lightgreen\">obs_resolution, obs_error, nens, spinup_length, spinup</span>: data assimilation settings (i.e., observation timestep, observation error, number of ensemble, whether it is spinup, **to be revised**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prepare PFLOTRAN input deck...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python /Users/jian449/Codes/DART/manhattan/models/pflotran/utils/prepare_pflotran_inputdeck.py /Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min/work/config.nml', returncode=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$prep_pflotran_inputdeck\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Prepare PFLOTRAN input deck...\")\n",
    "subprocess.run(\"python {} {}\".format(prep_pflotran_inputdeck, config_file), shell=True, check=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$pflotran_in\"', 'cat $1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='pflotran_spinup'></a>\n",
    " # Step 3: PFLOTRAN model spin-up\n",
    " Take in the ```pflotran.in``` and ```parameter.h5``` files and conduct the model spin-up by running ```pflotran.sh``` file. The ```pflotran.sh``` is a simple shell script executing ensemble simulation of PFLOTRAN by using MPI.\n",
    "\n",
    " **Run the code**\n",
    " - Run: ```prepare_pflotran_inpara.py```\n",
    " - Code input arguments (loaded from the configuration file):\n",
    "     - <span style=\"background-color:lightgreen\">pflotran_exe</span>: location of the executable PFLOTRAN\n",
    "     - <span style=\"background-color:lightgreen\">pflotran_in</span>: filename for ```pflotran.in```\n",
    "     - <span style=\"background-color:lightgreen\">pflotran_out_dir</span>: directory of PFLOTRAN output\n",
    "     - <span style=\"background-color:lightgreen\">nens</span>: number of ensemble\n",
    "     - <span style=\"background-color:lightgreen\">mpi_exe, ncore</span>: location of mpirun and number of cpu cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pflotran_sh, pflotran_out_dir = files_cfg[\"pflotran_sh_file\"], dirs_cfg[\"pflotran_out_dir\"]\n",
    "run_pflotran, pflotran_out_dir = files_cfg[\"run_pflotran_file\"], dirs_cfg[\"pflotran_out_dir\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Model spinup and run the forward simulation in the first assimilation time window...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python /Users/jian449/Codes/DART/manhattan/models/pflotran/utils/run_pflotran.py /Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min/work/config.nml', returncode=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Model spinup and run the forward simulation in the first assimilation time window...\")\n",
    "# subprocess.run(\"{} {}\".format(pflotran_sh, config_file), shell=True, check=True)\n",
    "subprocess.run(\"python {} {}\".format(run_pflotran, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ****************\n",
    " **Once the model spinup finishes, modify the corresponding configuration entry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs[\"time_cfg\"][\"is_spinup_done\"] = True\n",
    "configs.write(config_file, force=True)\n",
    "\n",
    "# raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='dart_prepare'></a>\n",
    " # Step 4: DART files preparation\n",
    " In this section, the following procedures are performed:\n",
    " - generate the template for DART generic variable quantity files (i.e., ```DEFAULT_obs_kind_mod.F90``` and ```obs_def_pflotran_mod.f90```);\n",
    " - generate the DART input namelists;\n",
    " - generate DART prior NetCDF data ```prior_ensemble_[ENS].nc``` from PFLOTRAN's parameter and outputs;\n",
    " - generate DART posterior NetCDF files (*sharing the same variable names and dimensions as the prior NetCDF files but without the data values*);\n",
    " - convert the observation file to DART observation format;\n",
    " - check ```model_mod.F90``` based on current setting by using the ```check_model_mod``` provided by DART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='dart_generic_prepare'></a>\n",
    " ## Generate the templates for DART generic variable quantity files\n",
    " - Run: ```list2dartqty.py``` to sequentially generate\n",
    "     - a mapping between PFLOTRAN variales and DART generic quantities in ```obs_def_pflotran_mod.F90```\n",
    "     - the default DART generic quantity definition file ```DEFAULT_obs_kind_mod.F90```\n",
    " - Code input arguments:\n",
    "     - <span style=\"background-color:lightgreen\">obs_type</span>: filename for ```DEFAULT_obs_kind_mod.F90```\n",
    "     - <span style=\"background-color:lightgreen\">def_obs_kind</span>: filename for ```obs_def_pflotran_mod.F90```\n",
    "     - <span style=\"background-color:lightgreen\">pflotran_parastate_set</span>: a list of variables required to be assimilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dartqty, obs_type_file = files_cfg[\"to_dartqty_file\"], files_cfg[\"obs_type_file\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Add PFLOTRAN variables to DEFAULT_obs_kind_mod.F90 if necessary...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python /Users/jian449/Codes/DART/manhattan/models/pflotran/utils/list2dartqty.py /Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min/work/config.nml', returncode=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$to_dartqty\" \"$config_file\"', 'python $1 $2 $3 $4')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Add PFLOTRAN variables to DEFAULT_obs_kind_mod.F90 if necessary...\")\n",
    "subprocess.run(\"python {} {}\".format(to_dartqty, config_file), shell=True, check=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Generate  DART input namelists in ```input.nml```\n",
    "\n",
    " The ```input.nml``` file is generated based on a template ```input.nml.template``` by modifying the following namelist entries:\n",
    "\n",
    " ```input.nml.template``` $\\rightarrow$ ```input.nml```\n",
    "\n",
    " |filter_nml|obs_kind_nml|preprocess_nml|model_nml|convertnc_nml|\n",
    " |:--:|:--:|:--:|:--:|:--:|\n",
    " | input_state_file_list, output_state_file_list, ens_size, async, adv_ens_command, obs_sequence_in_name | assimilate_these_obs_types | input_files, input_obs_kind_mod_file | time_step_days, time_step_seconds, nvar, var_names, template_file, var_qtynames | netcdf_file, out_file |\n",
    "\n",
    " **Namelists from DART**\n",
    " - [filter_nml](https://www.image.ucar.edu/DAReS/DART/Manhattan/assimilation_code/modules/assimilation/filter_mod.html): namelist of the main module for driving ensemble filter assimilations\n",
    " - [obs_kind_nml](https://www.image.ucar.edu/DAReS/DART/Manhattan/assimilation_code/modules/observations/obs_kind_mod.html#Namelist): namelist for controling what observation types are to be assimilated\n",
    " - [preprocess_nml](https://www.image.ucar.edu/DAReS/Codes/DART/manhattan/assimilation_code/programs/preprocess/preprocess): namelist of the DART-supplied preprocessor program which creates observation kind and observation definition modules from a set of other specially formatted Fortran 90 files\n",
    "\n",
    " **Self-defined namelists**\n",
    " - model_nml: a self-defined namelist for providing the basic information in the model\n",
    "     - time_step_days, time_step_seconds: the assimilation time window\n",
    "     - template_file: the template prior NetCDF file for ```model_mod.F90``` to digest the spatial information of the model\n",
    "     - var_names: the original variable names\n",
    "     - var_qtynames: the corresponding DART variable quantities\n",
    "     - nvar: the number of variables\n",
    " - convertnc_nml: a self-defined namelist for providing the NetCDF observation file name and the DART observation file name used in ```convert_nc.f90```\n",
    "     - netcdf_file: the location of the NetCDF file containing the observation data\n",
    "     - out_file: the location of the DART observation file\n",
    "\n",
    " **Note that**\n",
    " - There are more namelists or other items in the above namelist in input.nml.template. Users can edit the below python dictionary ```inputnml``` to include their modifications.\n",
    " - Users can also include more namelists provided by DART by modifying ```inputnml```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***************\n",
    " **Assemble all the namelists in input.nml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for different namelists in input.nml\n",
    "filter_nml = {\"input_state_file_list\":files_cfg[\"dart_input_list_file\"],\n",
    "              \"output_state_file_list\":files_cfg[\"dart_output_list_file\"],\n",
    "              \"ens_size\":da_cfg[\"nens\"],\n",
    "              \"num_output_state_members\":da_cfg[\"nens\"],\n",
    "              \"obs_sequence_in_name\":files_cfg[\"obs_dart_file\"]}\n",
    "#               \"obs_window_days\":obs_window_days,\n",
    "#               \"obs_window_seconds\":obs_window_seconds}\n",
    "obs_kind_nml = {\"assimilate_these_obs_types\":obs_set}\n",
    "assim_tools_nml = {\"filter_kind\":2}\n",
    "model_nml = {\"time_step_days\":da_cfg[\"assim_window_days\"],\n",
    "             \"time_step_seconds\":da_cfg[\"assim_window_seconds\"],\n",
    "             \"nvar\":len(obs_set)+len(para_set),\n",
    "             \"var_names\":obs_set+para_set,\n",
    "             \"template_file\":files_cfg[\"dart_prior_template_file\"],\n",
    "             \"var_qtynames\":['QTY_PFLOTRAN_'+v for v in obs_set]+['QTY_PFLOTRAN_'+v for v in para_set]}\n",
    "preprocess_nml = {\"input_files\":files_cfg[\"obs_type_file\"],\n",
    "                  \"input_obs_kind_mod_file\":files_cfg[\"def_obs_kind_file\"]}\n",
    "convertnc_nml = {\"netcdf_file\": files_cfg[\"obs_nc_file\"],\n",
    "                 \"out_file\": files_cfg[\"obs_dart_file\"],\n",
    "                 \"obs_start_day\": da_cfg[\"assim_start_days\"],\n",
    "                 \"obs_start_second\": da_cfg[\"assim_start_seconds\"],\n",
    "                 \"obs_end_day\": da_cfg[\"assim_end_days\"],\n",
    "                 \"obs_end_second\":da_cfg[\"assim_end_seconds\"],\n",
    "                 \"inflation_alpha\":da_cfg[\"enks_mda_alpha\"][da_cfg[\"enks_mda_iteration_step\"]-1]}\n",
    "modelmodcheck_nml = {\"input_state_files\": files_cfg[\"dart_prior_template_file\"]}\n",
    "inputnml = {\"filter_nml\":filter_nml,\n",
    "            \"obs_kind_nml\":obs_kind_nml,\n",
    "            \"assim_tools_nml\":assim_tools_nml,\n",
    "            \"model_nml\":model_nml,\n",
    "            \"preprocess_nml\":preprocess_nml,\n",
    "            \"convert_nc_nml\":convertnc_nml,\n",
    "            \"model_mod_check_nml\":modelmodcheck_nml}\n",
    "\n",
    "\n",
    "configs[\"inputnml_cfg\"] = inputnml\n",
    "\n",
    "# Save the configurations\n",
    "configs.write(config_file, force=True)\n",
    "# with open(config_pickle, 'wb') as f:\n",
    "#     pickle.dump(configs, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***************\n",
    " **Run the code**\n",
    " - Run: ```prepare_inputnml.py```\n",
    " - Code input arguments:\n",
    "     - <span style=\"background-color:lightgreen\">input_nml</span>: the ```input.nml``` namelist file\n",
    "     - <span style=\"background-color:lightgreen\">input_nml_dict</span>: the ```inputnml.p``` pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_inputnml = files_cfg[\"prep_inputnml_file\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Generate input.nml file...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python /Users/jian449/Codes/DART/manhattan/models/pflotran/utils/prepare_input_nml.py /Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min/work/config.nml', returncode=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s  \"$prep_inputnml\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Generate input.nml file...\")\n",
    "subprocess.run(\"python {} {}\".format(prep_inputnml, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='observationconvertion'></a>\n",
    " ## Prepare the observation conversion to DART observation format\n",
    " In this section, we prepare the process of converting the observation data to DART format. We first convert observation data in raw format into NetCDF format. Then, a fortran script is prepared for the conversion from the NetCDF to to DART format. The structure of NetCDF file for recording observation file.\n",
    "\n",
    " | NetCDF dimensions |           NetCDF variables          |\n",
    " |:-----------------:|:-----------------------------------:|\n",
    " | time: 1           | time: shape(time)                   |\n",
    " | location: nloc    | location: shape(location)           |\n",
    " |                   | physical variable: shape(time,nloc) |\n",
    "\n",
    " **Note that**\n",
    " - if the time calendar follows *gregorian*, the time unit should be entered as ```seconds since YYYY-MM-DD HH:MM:SS```. Otherwise, put the time calender as *None* and time unit as ```second``` (make sure convert your measurement times to seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***************\n",
    " **Convert the raw csv temperature observations to NetCDF file**\n",
    " - Run: ```csv2nc.py```\n",
    " - Code input arguments:\n",
    "     - <span style=\"background-color:lightgreen\">obs_original</span>: filename for the original observed temperature file\n",
    "     - <span style=\"background-color:lightgreen\">obs_nc</span>: filename for the observation NetCDF file\n",
    "     - <span style=\"background-color:lightgreen\">assim_start_str</span>: the reference time to set zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_nc, obs_nc_original = files_cfg[\"csv_to_nc_file\"], files_cfg[\"obs_nc_original_file\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Convert raw observation data to NetCDF file...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python /Users/jian449/Codes/DART/manhattan/models/pflotran/utils/csv2nc.py /Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min/work/config.nml', returncode=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$csv_to_nc\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Convert raw observation data to NetCDF file...\")\n",
    "subprocess.run(\"python {} {}\".format(csv_to_nc, config_file), shell=True, check=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***************\n",
    " **Clip the NetCDF file based on the defined spatial and temporal domains**\n",
    "\n",
    " The NetCDF file generated in the previous step is further processed by selecting data observed in the required spatial and temporal domains\n",
    " - Run: ```clip_obs_nc.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_obs_nc, obs_nc = files_cfg[\"clip_obs_nc_file\"], files_cfg[\"obs_nc_file\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Clip the NetCDF file based on the defined spatial and temporal domains...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python /Users/jian449/Codes/DART/manhattan/models/pflotran/utils/clip_obs_nc.py /Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min/work/config.nml', returncode=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$clip_obs_nc\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Clip the NetCDF file based on the defined spatial and temporal domains...\")\n",
    "subprocess.run(\"python {} {}\".format(clip_obs_nc, config_file), shell=True, check=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***************\n",
    " **Prepare the ```convert_nc.f90``` based on the list of observation variables**\n",
    " - Run: ```prepare_convert_nc.py```\n",
    " - Code input arguments:\n",
    "     - <span style=\"background-color:lightgreen\">obs_nc</span>: filename for the observation NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_convert_nc, convert_nc_file = files_cfg[\"prep_convert_nc_file\"], files_cfg[\"convert_nc_file\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Prepare the convert_nc.f90 based on the list of observation variables to be assimilated...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python /Users/jian449/Codes/DART/manhattan/models/pflotran/utils/prepare_convert_nc.py /Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min/work/config.nml', returncode=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$prep_convert_nc\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Prepare the convert_nc.f90 based on the list of observation variables to be assimilated...\")\n",
    "subprocess.run(\"python {} {}\".format(prep_convert_nc, config_file), shell=True, check=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='dart_executables'></a>\n",
    " # Step 5: Generate all the executable files\n",
    " Now, we compile all the executables from ```mkmf_*```. The following executables are generated here:\n",
    " - ```preprocess```: for preprocessing the [prepared DART generic variable quantity files prepared](#dart_generic_prepare)\n",
    " - ```convert_nc```: for [converting the observations from NetCDF to DART format](#observationconvertion)\n",
    " - ```model_mod_check```: for checking ```model_mod.F90``` interface file\n",
    " - ```filter```: for conducting the [DART data assimilation](https://www.image.ucar.edu/DAReS/DART/Manhattan/assimilation_code/programs/filter/filter.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Generate the executables\n",
    " - Run: ```quickbuild.csh```\n",
    " - Code input arguments:\n",
    "     - <span style=\"background-color:lightgreen\">app_work_dir</span>: location of the application work folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dart_work_dir, app_work_dir = dirs_cfg[\"dart_work_dir\"], dirs_cfg[\"app_work_dir\"]\n",
    "quickbuild = files_cfg[\"quickbuild_csh\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Generate all the executables...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='cd /Users/jian449/Codes/DART/manhattan/models/pflotran/work; csh /Users/jian449/Codes/DART/manhattan/models/pflotran/work/quickbuild.csh /Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min/work -mpi', returncode=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Generate all the executables...\")\n",
    "subprocess.run(\"cd {}; csh {} {} -mpi\".format(dart_work_dir, quickbuild, app_work_dir), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Check ```model_mod.F90``` interface file\n",
    " - Run: ```model_mod_check```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mod_check = files_cfg[\"model_mod_check_exe\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$app_work_dir\" \"$model_mod_check\"', 'cd $1\\n$2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='run_dart_pflotran'></a>\n",
    " # Step 6: Run DART and PFLOTRAN\n",
    " In this section, run the shell script to couple DART and PFLOTRAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dart_work_dir     = dirs_cfg[\"dart_work_dir\"]\n",
    "run_DART_PFLOTRAN = files_cfg[\"run_filter_csh\"]\n",
    "concatenate_output = files_cfg[\"concatenate_dart_output_file\"]\n",
    "inputnml_file     = files_cfg[\"input_nml_file\"]\n",
    "start_time        = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Assimilation starts here...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='cd /Users/jian449/Codes/DART/manhattan/models/pflotran/work; csh /Users/jian449/Codes/DART/manhattan/models/pflotran/work/run_DART_PFLOTRAN.csh /Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min/work/input.nml /Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min/work/config.nml', returncode=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$dart_work_dir\" \"$inputnml_file\" \"$config_file\"', 'cd $1\\ncsh run_DART_PFLOTRAN.csh $2 $3')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Assimilation starts here...\")\n",
    "subprocess.run(\"cd {}; csh {} {} {}\".format(dart_work_dir, run_DART_PFLOTRAN, inputnml_file, config_file), shell=True, check=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Concatenate the prior and posterior at all times ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='python /Users/jian449/Codes/DART/manhattan/models/pflotran/utils/concatenate_dart_output.py /Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_flux_reso5min/work/config.nml', returncode=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$dart_work_dir\" \"$inputnml_file\" \"$config_file\"', 'cd $1\\ncsh run_DART_PFLOTRAN.csh $2 $3')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Concatenate the prior and posterior at all times ...\")\n",
    "subprocess.run(\"python {} {}\".format(concatenate_output, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time usage of running DART and PFLOTRAN is 17.819 (second): \n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(\"The total time usage of running DART and PFLOTRAN is %.3f (second): \" % (end_time-start_time))\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
