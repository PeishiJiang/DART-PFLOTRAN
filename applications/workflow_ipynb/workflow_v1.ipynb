{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\">Overview</a></span></li><li><span><a href=\"#Step-1:-Configuration\" data-toc-modified-id=\"Step-1:-Configuration-2\">Step 1: Configuration</a></span></li><li><span><a href=\"#Step-2:-PFLOTRAN-preparation\" data-toc-modified-id=\"Step-2:-PFLOTRAN-preparation-3\">Step 2: PFLOTRAN preparation</a></span></li><li><span><a href=\"#Step-3:-PFLOTRAN-model-spin-up\" data-toc-modified-id=\"Step-3:-PFLOTRAN-model-spin-up-4\">Step 3: PFLOTRAN model spin-up</a></span></li><li><span><a href=\"#Step-4:-DART-files-preparation\" data-toc-modified-id=\"Step-4:-DART-files-preparation-5\">Step 4: DART files preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-the-templates-for-DART-generic-variable-quantity-files\" data-toc-modified-id=\"Generate-the-templates-for-DART-generic-variable-quantity-files-5.1\">Generate the templates for DART generic variable quantity files</a></span></li><li><span><a href=\"#Generate--DART-input-namelists-in-input.nml\" data-toc-modified-id=\"Generate--DART-input-namelists-in-input.nml-5.2\">Generate  DART input namelists in <code>input.nml</code></a></span></li><li><span><a href=\"#Prepare-the-observation-conversion-to-DART-observation-format\" data-toc-modified-id=\"Prepare-the-observation-conversion-to-DART-observation-format-5.3\">Prepare the observation conversion to DART observation format</a></span></li></ul></li><li><span><a href=\"#Step-5:-Generate-all-the-executable-files\" data-toc-modified-id=\"Step-5:-Generate-all-the-executable-files-6\">Step 5: Generate all the executable files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-the-executables\" data-toc-modified-id=\"Generate-the-executables-6.1\">Generate the executables</a></span></li><li><span><a href=\"#Check-model_mod.F90-interface-file\" data-toc-modified-id=\"Check-model_mod.F90-interface-file-6.2\">Check <code>model_mod.F90</code> interface file</a></span></li></ul></li><li><span><a href=\"#Step-6:-Run-DART-and-PFLOTRAN\" data-toc-modified-id=\"Step-6:-Run-DART-and-PFLOTRAN-7\">Step 6: Run DART and PFLOTRAN</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Overview\n",
    "  The **objective** of this notebook is to present the workflow of conducting data assimilation on [PFLOTRAN](https://www.pflotran.org/) by using [DART](https://www.image.ucar.edu/DAReS/DART/). Briefly, the procedures are as follows:\n",
    "  - [x] [Configuration](#parameter): define directories, file locations, and other parameters\n",
    "  - [x] [PFLOTRAN preparation](#pflotran_prepare): generate PFLOTRAN input files\n",
    "  - [x] [PFLOTRAN model spin-up](#pflotran_spinup): conduct model spin-up\n",
    "  - [x] [DART files preparation](#dart_prepare): add new DART quantities, prepare DART input namelists, prepare DART prior data, prepare observations in DART format, and check ```model_mod``` interface\n",
    "  - [x] [Generate all the executable files](#dart_executables): generate all the executables, convert observations in DART format, check ```model_mod``` interface, and test the data assimilation engine\n",
    "  - [x] [Run DART and PFLOTRAN](#run_dart_pflotran): run the shell script for integrating DART and PFLOTRAN model\n",
    "\n",
    "  Here, we perform inverse modeling on a 1D thermal model for illustration. The model assimilates temperature observation to update its parameters (i.e., flow flux)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <a id='parameter'></a>\n",
    "  # Step 1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import f90nml\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from math import floor, ceil\n",
    "from datetime import datetime, timedelta\n",
    "# get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "# get_ipython().run_line_magic('autoreload', '2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Define the locations of MPI, PFLOTRAN, application folder and DART-PFLOTRAN interface folder**\n",
    "\n",
    "  It is suggested that <span style=\"background-color:lightgreen\">mpi_exe</span> is defined based on the mpi utility (e.g., mpirun) installed by PFLOTRAN.\n",
    "\n",
    "  **Important:** You must make sure the <span style=\"background-color:lightgreen\">mpi_exe</span> has the same MPI system as the settings ```MPIFC``` and ```MPILD``` in ```mkmf.template```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPI settings\n",
    "# mpi_exe_da  = '/usr/local/bin/mpirun'  # The location of mpirun\n",
    "mpi_exe_da  = '/Users/jian449/Codes/petsc/arch-darwin-c-opt/bin/mpirun'\n",
    "mpi_exe_pf  = '/Users/jian449/Codes/petsc/arch-darwin-c-opt/bin/mpirun'\n",
    "# mpi_exe_da  = '/software/petsc_v3.11.3/arch-linux2-c-opt/bin/mpirun'  # The location of mpirun\n",
    "# mpi_exe_pf  = '/software/petsc_v3.11.3/arch-linux2-c-opt/bin/mpirun'\n",
    "ncore_da = 3  # The number of MPI cores used by DART\n",
    "ncore_pf = 4  # The number of MPI cores used by PFLOTRAN\n",
    "ngroup_pf= 4  # The number of group used by stochastic running in PFLOTRAN\n",
    "\n",
    "# PFLOTRAN executable\n",
    "# pflotran_exe  = '/global/project/projectdirs/m1800/pin/pflotran-haswell/src/pflotran/pflotran'\n",
    "pflotran_exe  = '/Users/jian449/Codes/pflotran/src/pflotran/pflotran'\n",
    "# pflotran_exe  = '/software/pflotran/src/pflotran/pflotran'\n",
    "\n",
    "# Main directory names\n",
    "temp_app_dir = \"/Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal\"          # The template for application folder\n",
    "app_dir      = \"/Users/jian449/Codes/DART/manhattan/models/pflotran/applications/1dthermal_parastate_1month_1mda\"          # The application folder name\n",
    "dart_dir     = \"/Users/jian449/Codes/DART/manhattan\"\n",
    "dart_pf_dir  = \"/Users/jian449/Codes/DART/manhattan/models/pflotran\"     # The dart pflotran utitlity folder name\n",
    "# temp_app_dir = \"/home/jian449/DART/manhattan/models/pflotran/applications/template\"          # The template for application folder\n",
    "# app_dir      = \"/home/jian449/DART/manhattan/models/pflotran/applications/1dthermal_test_1month_4mda_v2\"          # The application folder name\n",
    "# dart_dir     = \"/home/jian449/DART/manhattan\"\n",
    "# dart_pf_dir  = \"/home/jian449/DART/manhattan/models/pflotran\"     # The dart pflotran utitlity folder name\n",
    "# temp_app_dir = \"/global/cscratch1/sd/peishi89/DART_PFLOTRAN_APP/applications/template\"          # The template for application folder\n",
    "# app_dir      = \"/global/cscratch1/sd/peishi89/DART_PFLOTRAN_APP/applications/1dthermal\"          # The application folder name\n",
    "# dart_dir     = \"/global/homes/p/peishi89/DART/manhattan\"\n",
    "# dart_pf_dir  = \"/global/homes/p/peishi89/DART/manhattan/models/pflotran\"     # The dart pflotran utitlity folder name\n",
    "#temp_app_dir = os.path.abspath(\"../template\" )          # The template for application folder\n",
    "#app_dir      = os.path.abspath(\"../1dthermal/\")          # The application folder name\n",
    "#dart_dir     = os.path.abspath(\"../../../../\")\n",
    "#dart_pf_dir  = os.path.join(dart_dir, \"models/pflotran\")     # The dart pflotran utitlity folder name\n",
    "\n",
    "# configs = {}\n",
    "configs = f90nml.namelist.Namelist()\n",
    "configs[\"main_dir_cfg\"] = {\"app_dir\": app_dir, \"dart_dir\": dart_dir, \"dart_pf_dir\": dart_pf_dir}\n",
    "configs[\"exe_cfg\"]      = {\"pflotran_exe\": pflotran_exe, \"mpi_exe_da\": mpi_exe_da, \"mpi_exe_pf\": mpi_exe_pf,\n",
    "                           \"ncore_pf\": ncore_pf, \"ncore_da\": ncore_da, \"ngroup_pf\": ngroup_pf}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Generate the application directory if it does not exit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the application directory if it does not exists\n",
    "if not os.path.isdir(app_dir):\n",
    "    shutil.copytree(temp_app_dir, app_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Load all the required file paths/names from ```file_paths.nml```**\n",
    "\n",
    "  ```file_paths.nml``` defines the relative locations of all the files (e.g., utility files, shell scripts, data files, etc) used by this application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(dart_pf_dir)\n",
    "from utils.read_filepaths_nml import read_filepaths_nml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs_cfg, files_cfg      = read_filepaths_nml(app_dir=app_dir, dart_pf_dir=dart_pf_dir)\n",
    "files_cfg[\"keep_each_ens_file\"] = False   # Whether each ensemble nc file is kept when the DA ends\n",
    "configs[\"other_dir_cfg\"] = dirs_cfg\n",
    "configs[\"file_cfg\" ]     = files_cfg\n",
    "config_file              = files_cfg[\"config_file\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Clean up the folders under the app_dir**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pflotran_in_dir   = dirs_cfg[\"pflotran_in_dir\"]\n",
    "pflotran_out_dir  = dirs_cfg[\"pflotran_out_dir\"]\n",
    "dart_inout_dir    = dirs_cfg[\"dart_data_dir\"]\n",
    "# subprocess.run(\"cd {}; rm *\".format(pflotran_in_dir), shell=True, check=False)\n",
    "subprocess.run(\"cd {}; rm -f *\".format(pflotran_out_dir), shell=True, check=True)\n",
    "subprocess.run(\"cd {}; ls | xargs rm\".format(dart_inout_dir), shell=True, check=True)\n",
    "# subprocess.run(\"cd {}; ls | xargs -r rm\".format(dart_inout_dir), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Specify the following types of variables used in DART**\n",
    "  - the observation data to be assimilated\n",
    "  - the PFLOTRAN parameters to be analyzed\n",
    "  - the statistics of the parameters: (1) the value range; (2) the mean and std for randomly sampling; (3) the distribution to be sampled (i.e., normal, log normal, and uniform distributions)\n",
    "  - Whether the prior at one time window is rescaled from the posterior from the previous window\n",
    "  - The list of parameters whose prior would be resampled based on the mean of the corresponding posterior at the previous time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation data to be assimilated\n",
    "obs_set  = ['TEMPERATURE']\n",
    "\n",
    "# The PFLOTRAN parameters to be analyzed\n",
    "# para_set = ['FLOW_FLUX','POROSITY','THERMAL_CONDUCTIVITY']\n",
    "para_set = ['FLOW_FLUX']\n",
    "para_homogeneous = True\n",
    "\n",
    "# The statistics of the parameters\n",
    "# The index order follows para_set\n",
    "# para_min_set  = [-10.0, 0.01, 0.5]  # The minimum values (-99999 means no lower bound limit)\n",
    "# para_max_set  = [10.0, 0.7, 2.5]  # The maximum values (99999 means no upper bound limit)\n",
    "# para_mean_set = [0.0, 0.3, 1.5]  # The mean values\n",
    "# para_std_set  = [0.5, 0.1, 0.5]  # The standard deviation values\n",
    "# para_dist_set = [\"normal\", \"normal\", \"normal\"]  # The assumed distribution to be sampled\n",
    "para_min_set  = [-5.0]  # The minimum values (-99999 means no lower bound limit)\n",
    "para_max_set  = [5.0]  # The maximum values (99999 means no upper bound limit)\n",
    "para_mean_set = [0.0]  # The mean values\n",
    "para_std_set  = [0.5]  # The standard deviation values\n",
    "para_dist_set = [\"normal\"]  # The assumed distribution to be sampled\n",
    "para_prior_rescaled = False  # Whether the prior is generated by rescaling the posterior\n",
    "\n",
    "para_resampled_set = ['FLOW_FLUX']   # The parameters to be resampled at each time step\n",
    "# para_resampled_set = ['']   # The parameters to be resampled at each time step\n",
    "\n",
    "configs[\"obspara_set_cfg\"] = {\"obs_set\": obs_set, \"para_set\": para_set, \n",
    "                              \"para_prior_rescaled\": para_prior_rescaled, \"para_homogeneous\": para_homogeneous,\n",
    "                              \"para_min_set\": para_min_set, \"para_max_set\": para_max_set,\n",
    "                              \"para_mean_set\": para_mean_set, \"para_std_set\": para_std_set,\n",
    "                              \"para_dist_set\": para_dist_set, \"para_resampled_set\": para_resampled_set}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Specify the spatial domains of the observation data to be assimilated**\n",
    "\n",
    "  The limits of x, y, and z are bounded by the minimum and maximum boundaries through (min, max). If the limit is not specified, -99999 and 99999 are assigned for the lower and upper bounds, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space_xlimit = [-99999, 99999]\n",
    "obs_space_ylimit = [-99999, 99999]\n",
    "obs_space_zlimit = [-0.50, -0.03]\n",
    "configs[\"obs_space_cfg\"] = {\"obs_space_xlimit\": obs_space_xlimit, \"obs_space_ylimit\": obs_space_ylimit,\n",
    "                            \"obs_space_zlimit\": obs_space_zlimit}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Specify the data assimilation time window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_cfg = {}\n",
    "\n",
    "# Assimilation time window time_step_days+time_step_seconds\n",
    "# Assimilation window\n",
    "da_cfg[\"assim_window_days\"]    = 0     # assimilation time window/step (day)\n",
    "da_cfg[\"assim_window_seconds\"] = 3600  # assimilation time window/step  (second)\n",
    "da_cfg[\"assim_window_size\"] = da_cfg[\"assim_window_days\"]+float(da_cfg[\"assim_window_seconds\"])/86400. # day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Specify the temporal information**\n",
    "  - model spinup time/start time\n",
    "  - the current model time (considered as the middle time of the current assimilation time window)\n",
    "  - the list of model time\n",
    "  - the list of starting assimilation time (the first starting from the end of spinup)\n",
    "  - the map between the observation start time and model start time (considered as the end of spinup)\n",
    "  - the first and the last observation times\n",
    "\n",
    "  **note that** model start time is considered after the spinup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cfg = {}\n",
    "one_sec = 1./86400. # fraction of day\n",
    "# Model spinup length\n",
    "time_cfg[\"spinup_length_days\"] = 0    # number of days for spinup\n",
    "time_cfg[\"spinup_length_seconds\"] = 7200    # number of seconds for the remaining spinup\n",
    "time_cfg[\"spinup_length\"]  = time_cfg[\"spinup_length_days\"]+float(time_cfg[\"spinup_length_seconds\"])/86400. # spinup time (day)\n",
    "time_cfg[\"is_spinup_done\"] = False  # whether spinup is conducted\n",
    "\n",
    "# Model start time\n",
    "time_cfg[\"current_model_time\"] = time_cfg[\"spinup_length\"] + (da_cfg[\"assim_window_size\"] + one_sec)/2  # model start time should be the middle of the first assimilation window (after spinup)\n",
    "# time_cfg[\"model_time_list\"]    = [0.]   # the list of model time\n",
    "time_cfg[\"model_time_list\"]    = [time_cfg[\"current_model_time\"]]   # the list of model time\n",
    "\n",
    "# Map between observation assimilation start time and model start time\n",
    "obs_start   = datetime(2017,4,1,0,0,0)\n",
    "assim_start = obs_start + timedelta(days=time_cfg[\"spinup_length\"]) # assimilation time should be after the model spinup\n",
    "time_cfg[\"model_start\"] = obs_start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "time_cfg[\"assim_start\"] = assim_start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# The maximum time for observation\n",
    "time_cfg[\"first_obs_time_days\"]    = time_cfg[\"spinup_length_days\"]\n",
    "time_cfg[\"first_obs_time_seconds\"] = time_cfg[\"spinup_length_seconds\"]\n",
    "time_cfg[\"first_obs_time_size\"] = time_cfg[\"first_obs_time_days\"]+float(time_cfg[\"first_obs_time_seconds\"])/86400. # day\n",
    "time_cfg[\"last_obs_time_days\"]    = time_cfg[\"first_obs_time_days\"]\n",
    "time_cfg[\"last_obs_time_seconds\"] = time_cfg[\"first_obs_time_seconds\"] + 3600*24*29\n",
    "# time_cfg[\"last_obs_time_seconds\"] = time_cfg[\"first_obs_time_seconds\"] + 3600*100\n",
    "time_cfg[\"last_obs_time_size\"] = time_cfg[\"last_obs_time_days\"]+float(time_cfg[\"last_obs_time_seconds\"])/86400. # day\n",
    "\n",
    "# Whether the model time exceeds the last observation\n",
    "time_cfg[\"exceeds_obs_time\"] = time_cfg[\"current_model_time\"] >= time_cfg[\"last_obs_time_size\"]\n",
    "\n",
    "# Save them to configs\n",
    "configs[\"time_cfg\"] = time_cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Config the data assimilation**\n",
    "  - observation error and error type\n",
    "  - number of realizations\n",
    "  - the start and end time of the current assimilation time window (obtained based on the previous temporal information settings)\n",
    "  - decision on whether the observation posteriors are recomputed based on the updated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation error, number of ensembles\n",
    "da_cfg[\"obs_reso\"]  = 300.0      # observation resolution (second)\n",
    "da_cfg[\"nens\"]      = 100        # number of ensembles\n",
    "da_cfg[\"obs_error\"] = 0.05       # the observation error\n",
    "da_cfg[\"obs_error_type\"] = \"absolute\" # the type of observation error (i.e., relative and absolute)\n",
    "\n",
    "# The start and end time of the current assimilation time window\n",
    "da_cfg[\"assim_start_days\"]    = int(floor(time_cfg[\"current_model_time\"] - da_cfg[\"assim_window_size\"]/2))\n",
    "da_cfg[\"assim_start_seconds\"] = int((time_cfg[\"current_model_time\"] - da_cfg[\"assim_window_size\"]/2 - da_cfg[\"assim_start_days\"])*86400+1)\n",
    "da_cfg[\"assim_end_days\"]      = int(floor(time_cfg[\"current_model_time\"] + da_cfg[\"assim_window_size\"]/2))\n",
    "da_cfg[\"assim_end_seconds\"]   = int((time_cfg[\"current_model_time\"] + da_cfg[\"assim_window_size\"]/2 - da_cfg[\"assim_end_days\"])*86400)\n",
    "\n",
    "# Compute the number of time steps\n",
    "# print(np.ceil((time_cfg[\"last_obs_time_size\"] - time_cfg[\"first_obs_time_size\"]) / da_cfg[\"assim_window_size\"]))\n",
    "da_cfg[\"ntimestep\"] = ceil((time_cfg[\"last_obs_time_size\"] - time_cfg[\"first_obs_time_size\"]) / da_cfg[\"assim_window_size\"])\n",
    "\n",
    "# Decide whether the observation posterior should be recomputed from the model output based on the parameter posterior\n",
    "da_cfg[\"obs_ens_posterior_from_model\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Config the MDA settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inflation settings used in EnKS-MDA (the alpha value)\n",
    "# da_cfg[\"enks_mda_alpha\"] = [4., 4., 4., 4.]  # Note that the summation of the inverse of alpha should be one\n",
    "# da_cfg[\"enks_mda_alpha\"] = [3., 3., 3.]  # Note that the summation of the inverse of alpha should be one\n",
    "# da_cfg[\"enks_mda_alpha\"] = [2., 2.]  # Note that the summation of the inverse of alpha should be one\n",
    "da_cfg[\"enks_mda_alpha\"] = [1.]  # Note that the summation of the inverse of alpha should be one\n",
    "da_cfg[\"enks_mda_iteration_step\"] = 1  # the ith iteration (1 for the first iteration)\n",
    "da_cfg[\"enks_mda_total_iterations\"] = len(da_cfg[\"enks_mda_alpha\"])  # Note that the summation of the inverse of alpha should be one\n",
    "\n",
    "# Check whether the sum of the inverse of enks_mda_alpha is one\n",
    "alpha_inv_sum = sum([1./alpha for alpha in da_cfg[\"enks_mda_alpha\"]])\n",
    "if alpha_inv_sum - 1 > 1e-8:\n",
    "    raise Exception(\"The sum of the inverse of enks_mda_alpha should be one!\")\n",
    "\n",
    "# Save them to configs\n",
    "configs[\"da_cfg\"] = da_cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Save all the configurations in pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.write(config_file, force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <a id='pflotran_prepare'></a>\n",
    "  # Step 2: PFLOTRAN preparation\n",
    "  *Here, we use Kewei's 1D thermal model as an example for generating PFLOTRAN input card and parameter.h5.*\n",
    "\n",
    "  In this section, the following procedures are performed:\n",
    "  - generate PFLOTRAN input deck file ```PFLOTRAN.in```\n",
    "  - generate the parameter files in HDF 5, ```parameter_prior.h5```, used by PFLOTRAN input deck file\n",
    "\n",
    "  **Note that**\n",
    "  - ```PFLOTRAN.in``` for each DA scenario should be prepared by users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_pflotran_inputdeck, pflotran_in = files_cfg[\"prep_pflotran_inputdeck_file\"], files_cfg[\"pflotran_in_file\"]\n",
    "prep_pflotran_parameterprior = files_cfg[\"prep_pflotran_para_file\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Generate the ensembles of PFLOTRAN prior**\n",
    "\n",
    "  **Run code**\n",
    "  - Run: ```prepare_pflotran_parameterprior.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$prep_pflotran_parameterprior\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Prepare PFLOTRAN ensemble parameter values...\")\n",
    "subprocess.run(\"python {} {}\".format(prep_pflotran_parameterprior, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Generate PFLOTRAN input deck file**\n",
    "\n",
    "  **Run code**\n",
    "  - Run: ```prepare_pflotran_inputdeck.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$prep_pflotran_inputdeck\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Prepare PFLOTRAN input deck...\")\n",
    "subprocess.run(\"python {} {}\".format(prep_pflotran_inputdeck, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <a id='pflotran_spinup'></a>\n",
    "  # Step 3: PFLOTRAN model spin-up\n",
    "  Take in the ```pflotran.in``` and ```parameter.h5``` files and conduct the model spin-up by running ```pflotran.sh``` file. The ```pflotran.sh``` is a simple shell script executing ensemble simulation of PFLOTRAN by using MPI.\n",
    "\n",
    "  **Run the code**\n",
    "  - Run: ```run_pflotran.py```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pflotran_sh, pflotran_out_dir = files_cfg[\"pflotran_sh_file\"], dirs_cfg[\"pflotran_out_dir\"]\n",
    "run_pflotran, pflotran_out_dir = files_cfg[\"run_pflotran_file\"], dirs_cfg[\"pflotran_out_dir\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Model spinup and run the forward simulation in the first assimilation time window...\")\n",
    "# subprocess.run(\"{} {}\".format(pflotran_sh, config_file), shell=True, check=True)\n",
    "subprocess.run(\"python {} {}\".format(run_pflotran, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ****************\n",
    "  **Once the model spinup finishes, modify the corresponding configuration entry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs[\"time_cfg\"][\"is_spinup_done\"] = True\n",
    "configs.write(config_file, force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <a id='dart_prepare'></a>\n",
    "  # Step 4: DART files preparation\n",
    "  In this section, the following procedures are performed:\n",
    "  - [TO BE REVISED]: generate the template for DART generic variable quantity files (i.e., ```DEFAULT_obs_kind_mod.F90``` and ```obs_def_pflotran_mod.f90```);\n",
    "  - generate the DART input namelists;\n",
    "  - generate DART prior NetCDF data ```prior_ensemble_[ENS].nc``` from PFLOTRAN's parameter and outputs;\n",
    "  - generate DART posterior NetCDF files (*sharing the same variable names and dimensions as the prior NetCDF files but without the data values*);\n",
    "  - convert the observation file to DART observation format;\n",
    "  - check ```model_mod.F90``` based on current setting by using the ```check_model_mod``` provided by DART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <a id='dart_generic_prepare'></a>\n",
    "  ## Generate the templates for DART generic variable quantity files\n",
    "  - Run: ```list2dartqty.py``` to sequentially generate/modify\n",
    "      - a mapping between PFLOTRAN variales and DART generic quantities in ```obs_def_pflotran_mod.F90```\n",
    "      - the default DART generic quantity definition file ```DEFAULT_obs_kind_mod.F90```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dartqty, obs_type_file = files_cfg[\"to_dartqty_file\"], files_cfg[\"obs_type_file\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$to_dartqty\" \"$config_file\"', 'python $1 $2 $3 $4')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Add PFLOTRAN variables to DEFAULT_obs_kind_mod.F90 if necessary...\")\n",
    "subprocess.run(\"python {} {}\".format(to_dartqty, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Generate  DART input namelists in ```input.nml```\n",
    "\n",
    "  The ```input.nml``` file is generated based on a template ```input.nml.template``` by modifying the following namelist entries:\n",
    "\n",
    "  ```input.nml.template``` $\\rightarrow$ ```input.nml```\n",
    "\n",
    "  **Namelists from DART**\n",
    "  - [obs_kind_nml](https://www.image.ucar.edu/DAReS/DART/Manhattan/assimilation_code/modules/observations/obs_kind_mod.html#Namelist): namelist for controling what observation types are to be assimilated\n",
    "  - [preprocess_nml](https://www.image.ucar.edu/DAReS/Codes/DART/manhattan/assimilation_code/programs/preprocess/preprocess): namelist of the DART-supplied preprocessor program which creates observation kind and observation definition modules from a set of other specially formatted Fortran 90 files\n",
    "  - and others (see DART documentation for details.)\n",
    "\n",
    "  **Self-defined namelists**\n",
    "  - smoother_nml: a self-defined namelist of the main module for driving ensemble data assimilations modified from DART [filter_nml](https://www.image.ucar.edu/DAReS/DART/Manhattan/assimilation_code/modules/assimilation/filter_mod.html#Namelist)\n",
    "  - model_nml: a self-defined namelist for providing the basic information in the model\n",
    "      - template_file: the template prior NetCDF file for ```model_mod.F90``` to digest the spatial information of the model\n",
    "      - nvar_para: the number of parameter variables\n",
    "      - para_var_names: the original variable names for model parameters\n",
    "      - para_var_qtynames: the corresponding DART variable quantities for model parameters\n",
    "      - nvar_state: the number of state variables\n",
    "      - state_var_names: the original variable names for model states\n",
    "      - state_var_qtynames: the corresponding DART variable quantities for model states\n",
    "      - debug: whether the debug mode is turned on\n",
    "      - max_time_diff_seconds: the maximum time difference allowed for finding the closest ensemble for observations (second)\n",
    "      \n",
    "  - convertnc_nml: a self-defined namelist for providing the NetCDF observation file name and the DART observation file name used in ```convert_nc.f90```\n",
    "      - netcdf_file: the location of the NetCDF file containing the observation data\n",
    "      - out_file: the location of the DART observation file\n",
    "\n",
    "  **Note that**\n",
    "  - There are more namelists or other items in the above namelist in input.nml.template. Users can edit the below python dictionary ```inputnml``` to include their modifications.\n",
    "  - Users can also include more namelists provided by DART by modifying ```inputnml```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ***************\n",
    "  **Assemble all the namelists in input.nml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for different namelists in input.nml\n",
    "smoother_nml = {\"input_state_file_list\":files_cfg[\"dart_input_list_file\"],\n",
    "              \"output_state_file_list\":files_cfg[\"dart_output_list_file\"],\n",
    "              \"ens_size\":da_cfg[\"nens\"],\n",
    "              \"num_output_state_members\":da_cfg[\"nens\"],\n",
    "              \"obs_sequence_in_name\":files_cfg[\"obs_dart_file\"],\n",
    "              \"first_obs_days\": da_cfg[\"assim_start_days\"],\n",
    "              \"first_obs_seconds\": da_cfg[\"assim_start_seconds\"],\n",
    "              \"last_obs_days\": da_cfg[\"assim_end_days\"],\n",
    "              \"last_obs_seconds\": da_cfg[\"assim_end_seconds\"],\n",
    "              \"output_mean\": False,\n",
    "              \"output_sd\": False}\n",
    "obs_kind_nml = {\"assimilate_these_obs_types\":obs_set}\n",
    "assim_tools_nml = {\"filter_kind\":2}\n",
    "model_nml = {\"nvar_state\":len(obs_set),\n",
    "             \"state_var_names\":obs_set,\n",
    "             \"state_var_qtynames\":['QTY_PFLOTRAN_'+v for v in obs_set],\n",
    "             \"nvar_para\":len(para_set),\n",
    "             \"para_var_names\":para_set,\n",
    "             \"para_var_qtynames\":['QTY_PFLOTRAN_'+v for v in para_set],\n",
    "             \"max_time_diff_seconds\": 10,\n",
    "             \"debug\": False,\n",
    "             \"template_file\":files_cfg[\"dart_prior_template_file\"]}\n",
    "preprocess_nml = {\"input_files\":files_cfg[\"obs_type_file\"],\n",
    "                  \"input_obs_kind_mod_file\":files_cfg[\"def_obs_kind_file\"]}\n",
    "convertnc_nml = {\"netcdf_file\": files_cfg[\"obs_nc_file\"],\n",
    "                 \"out_file\": files_cfg[\"obs_dart_file\"],\n",
    "                 \"obs_start_day\": da_cfg[\"assim_start_days\"],\n",
    "                 \"obs_start_second\": da_cfg[\"assim_start_seconds\"],\n",
    "                 \"obs_end_day\": da_cfg[\"assim_end_days\"],\n",
    "                 \"obs_end_second\":da_cfg[\"assim_end_seconds\"],\n",
    "                 \"inflation_alpha\":da_cfg[\"enks_mda_alpha\"][da_cfg[\"enks_mda_iteration_step\"]-1]}\n",
    "modelmodcheck_nml = {\"input_state_files\": files_cfg[\"dart_prior_template_file\"]}\n",
    "inputnml = {\"smoother_nml\":smoother_nml,\n",
    "            \"obs_kind_nml\":obs_kind_nml,\n",
    "            \"assim_tools_nml\":assim_tools_nml,\n",
    "            \"model_nml\":model_nml,\n",
    "            \"preprocess_nml\":preprocess_nml,\n",
    "            \"convert_nc_nml\":convertnc_nml,\n",
    "            \"model_mod_check_nml\":modelmodcheck_nml}\n",
    "\n",
    "\n",
    "configs[\"inputnml_cfg\"] = inputnml\n",
    "\n",
    "# Save the configurations\n",
    "configs.write(config_file, force=True)\n",
    "# with open(config_pickle, 'wb') as f:\n",
    "#     pickle.dump(configs, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ***************\n",
    "  **Run the code**\n",
    "  - Run: ```prepare_inputnml.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_inputnml = files_cfg[\"prep_inputnml_file\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s  \"$prep_inputnml\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Generate input.nml file...\")\n",
    "subprocess.run(\"python {} {}\".format(prep_inputnml, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <a id='observationconvertion'></a>\n",
    "  ## Prepare the observation conversion to DART observation format\n",
    "  In this section, we prepare the process of converting the observation data to DART format. We first convert observation data in raw format into NetCDF format. Then, a fortran script is prepared for the conversion from the NetCDF to to DART format. The structure of NetCDF file for recording observation file.\n",
    "\n",
    "  | NetCDF dimensions |           NetCDF variables          |\n",
    "  |:-----------------:|:-----------------------------------:|\n",
    "  | time: ntime       | time: shape(time)                   |\n",
    "  | location: nloc    | location: shape(location)           |\n",
    "  |                   | physical variable: shape(time,nloc) |\n",
    "\n",
    "  **Note that**\n",
    "  - if the time calendar follows *gregorian*, the time unit should be entered as ```seconds since YYYY-MM-DD HH:MM:SS```. Otherwise, put the time calender as *None* and time unit as ```second``` (make sure convert your measurement times to seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ***************\n",
    "  **Convert the raw csv temperature observations to NetCDF file**\n",
    "  - Run: ```csv2nc.py```\n",
    "\n",
    "  **Note that**\n",
    "  - Here, we illustrate the conversion from csv as an example. However, the conversion to netCDF format can be arbitrary based on the raw data format. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_nc, obs_nc_original = files_cfg[\"csv_to_nc_file\"], files_cfg[\"obs_nc_original_file\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$csv_to_nc\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Convert raw observation data to NetCDF file...\")\n",
    "subprocess.run(\"python {} {}\".format(csv_to_nc, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ***************\n",
    "  **Clip the NetCDF file based on the defined spatial and temporal domains**\n",
    "\n",
    "  The NetCDF file generated in the previous step is further processed by selecting data observed in the required spatial and temporal domains\n",
    "  - Run: ```clip_obs_nc.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_obs_nc, obs_nc = files_cfg[\"clip_obs_nc_file\"], files_cfg[\"obs_nc_file\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$clip_obs_nc\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Clip the NetCDF file based on the defined spatial and temporal domains...\")\n",
    "subprocess.run(\"python {} {}\".format(clip_obs_nc, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ***************\n",
    "  **Prepare the ```convert_nc.f90``` based on the list of observation variables**\n",
    "  - Run: ```prepare_convert_nc.py```\n",
    "  - Code input arguments:\n",
    "      - <span style=\"background-color:lightgreen\">obs_nc</span>: filename for the observation NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_convert_nc, convert_nc_file = files_cfg[\"prep_convert_nc_file\"], files_cfg[\"convert_nc_file\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$prep_convert_nc\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Prepare the convert_nc.f90 based on the list of observation variables to be assimilated...\")\n",
    "subprocess.run(\"python {} {}\".format(prep_convert_nc, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <a id='dart_executables'></a>\n",
    "  # Step 5: Generate all the executable files\n",
    "  Now, we compile all the executables from ```mkmf_*```. The following executables are generated here:\n",
    "  - ```preprocess```: for preprocessing the [prepared DART generic variable quantity files prepared](#dart_generic_prepare)\n",
    "  - ```convert_nc```: for [converting the observations from NetCDF to DART format](#observationconvertion)\n",
    "  - ```model_mod_check```: for checking ```model_mod.F90``` interface file\n",
    "  - ```smoother```: for conducting the DART data assimilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Generate the executables\n",
    "  - Run: ```quickbuild.csh```\n",
    "  - Code input arguments:\n",
    "      - <span style=\"background-color:lightgreen\">app_work_dir</span>: location of the application work folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dart_work_dir, app_work_dir = dirs_cfg[\"dart_work_dir\"], dirs_cfg[\"app_work_dir\"]\n",
    "quickbuild = files_cfg[\"quickbuild_csh\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Generate all the executables...\")\n",
    "subprocess.run(\"cd {}; csh {} {} -mpi\".format(dart_work_dir, quickbuild, app_work_dir), shell=True, check=True)\n",
    "# subprocess.run(\"cd {}; csh {} {}\".format(dart_work_dir, quickbuild, app_work_dir), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Check ```model_mod.F90``` interface file\n",
    "  - Run: ```model_mod_check```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mod_check = files_cfg[\"model_mod_check_exe\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <a id='run_dart_pflotran'></a>\n",
    "  # Step 6: Run DART and PFLOTRAN\n",
    "  In this section, run the shell script to couple DART and PFLOTRAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dart_work_dir     = dirs_cfg[\"dart_work_dir\"]\n",
    "run_DART_PFLOTRAN = files_cfg[\"run_da_csh\"]\n",
    "concatenate_output = files_cfg[\"concatenate_dart_output_file\"]\n",
    "inputnml_file     = files_cfg[\"input_nml_file\"]\n",
    "start_time        = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Assimilation starts here...\")\n",
    "subprocess.run(\"cd {}; csh {} {} {}\".format(dart_work_dir, run_DART_PFLOTRAN, inputnml_file, config_file), shell=True, check=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Concatenate the prior and posterior at all times ...\")\n",
    "subprocess.run(\"python {} {}\".format(concatenate_output, config_file), shell=True, check=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(\"The total time usage of running DART and PFLOTRAN is %.3f (second): \" % (end_time-start_time))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
