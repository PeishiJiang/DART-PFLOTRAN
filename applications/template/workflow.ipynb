{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "   # Overview\n",
    "   The **objective** of this notebook is to present the workflow of conducting data assimilation on [PFLOTRAN](https://www.pflotran.org/) by using [DART](https://www.image.ucar.edu/DAReS/DART/). Briefly, the procedures are as follows:\n",
    "   - [x] [Configuration](#parameter): define directories, file locations, and other parameters\n",
    "   - [x] [PFLOTRAN preparation](#pflotran_prepare): generate PFLOTRAN input files and conduct model spin-up\n",
    "   - [x] [DART preparation](#dart_prepare): add new DART quantities, prepare DART input namelists, prepare DART prior data, prepare observations in DART format, and generate all the executables, convert observations in DART format, check ```model_mod``` interface, and test the data assimilation engine\n",
    "   - [x] [Perform the sequential ES-MDA](#run_dart_pflotran): run the sequential ES-MDA using DART-PFLOTRAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************\n",
    "**Note that** make sure you have installed:\n",
    "- the required packages (i.e., python packages and netcdf-fortran, see this [post])(https://gitlab.pnnl.gov/sbrsfa/dart-pflotran/-/blob/master/INSTALL_CONDA_VIRT_ENV.md));\n",
    "- DART manhattan version (see this [post](https://gitlab.pnnl.gov/sbrsfa/dart-pflotran/-/blob/master/INSTALL_DART.md));\n",
    "- and PFLOTRAN (see this [post](https://www.pflotran.org/documentation/user_guide/how_to/installation/linux.html#linux-install))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Step 1: Configuration\n",
    "<a id='parameter'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import f90nml\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from math import floor, ceil\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Define the locations of MPI, PFLOTRAN, application folder and DART-PFLOTRAN interface folder**\n",
    "\n",
    "   It is suggested that <span style=\"background-color:lightgreen\">mpi_exe</span> is defined based on the mpi utility (e.g., mpirun) installed by PFLOTRAN.\n",
    "\n",
    "   **Important:** You must make sure the <span style=\"background-color:lightgreen\">mpi_exe</span> has the same MPI system as the settings ```MPIFC``` and ```MPILD``` in ```mkmf.template```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPI settings\n",
    "mpi_exe_da  = 'srun'\n",
    "mpi_exe_pf  = 'srun'\n",
    "ncore_da = 100  # The number of MPI cores used by DART\n",
    "ncore_pf = 6100  # The number of MPI cores used by PFLOTRAN\n",
    "ngroup_pf= 100  # The number of group used by stochastic running in PFLOTRAN\n",
    "\n",
    "# PFLOTRAN executable\n",
    "pflotran_exe  = '/global/project/projectdirs/m1800/pin/pflotran/pflotran-dev/src/pflotran/bin/pflotran-dev'\n",
    "\n",
    "# Main directory names\n",
    "temp_app_dir = \"/global/project/projectdirs/m1800/peishi/300A_analysis/Richards_av_std1.5\"          # The template for application folder\n",
    "app_dir      = \"/global/cscratch1/sd/peishi89/300A-analysis/Richards_av_std1.5_per15days\"          # The template for application folder\n",
    "dart_dir     = \"/global/homes/p/peishi89/DART/manhattan\"\n",
    "dart_pf_dir  = \"/global/homes/p/peishi89/DART/manhattan/models/pflotran\"     # The dart pflotran utitlity folder name\n",
    "\n",
    "# configs = {}\n",
    "configs = f90nml.namelist.Namelist()\n",
    "configs[\"main_dir_cfg\"] = {\"app_dir\": app_dir, \"dart_dir\": dart_dir, \"dart_pf_dir\": dart_pf_dir}\n",
    "configs[\"exe_cfg\"]      = {\"pflotran_exe\": pflotran_exe, \"mpi_exe_da\": mpi_exe_da, \"mpi_exe_pf\": mpi_exe_pf,\n",
    "                           \"ncore_pf\": ncore_pf, \"ncore_da\": ncore_da, \"ngroup_pf\": ngroup_pf}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Generate the application directory if it does not exit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the application directory if it does not exists\n",
    "if not os.path.isdir(app_dir):\n",
    "    shutil.copytree(temp_app_dir, app_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Load all the required file paths/names from ```file_paths.nml```**\n",
    "\n",
    "   ```file_paths.nml``` defines the relative locations of all the files (e.g., utility files, shell scripts, data files, etc) used by this application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(dart_pf_dir)\n",
    "from utils.read_filepaths_nml import read_filepaths_nml\n",
    "dirs_cfg, files_cfg      = read_filepaths_nml(app_dir=app_dir, dart_pf_dir=dart_pf_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Set file saving options**\n",
    "   - keep_each_ens_file: whether each ensemble nc file read and generated by DART is kept when the DA ends.\n",
    "   - save_immediate_mda_result: whether the results of each MDA iteration is kept when the DA ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_cfg[\"keep_each_ens_file\"] = True   # Whether each ensemble nc file is kept when the DA ends\n",
    "files_cfg[\"save_immediate_mda_result\"] = True   # Indicate whether the results of each MDA iteration will be saved\n",
    "config_file              = files_cfg[\"config_file\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Clean up the folders under the application folder (i.e., app_dir)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pflotran_in_dir   = dirs_cfg[\"pflotran_in_dir\"]\n",
    "pflotran_out_dir  = dirs_cfg[\"pflotran_out_dir\"]\n",
    "dart_inout_dir    = dirs_cfg[\"dart_data_dir\"]\n",
    "# subprocess.run(\"cd {}; rm *\".format(pflotran_in_dir), shell=True, check=False)\n",
    "subprocess.run(\"cd {}; rm -f *\".format(pflotran_out_dir), shell=True, check=True)\n",
    "subprocess.run(\"cd {}; ls | xargs rm\".format(dart_inout_dir), shell=True, check=False)\n",
    "# subprocess.run(\"cd {}; ls | xargs -r rm\".format(dart_inout_dir), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Define PFLOTRAN-related and observation files**\n",
    "   - pflotran_para_file: file path for storing multiple realizations of parameters in HDF5 following [cell index option.](https://www.pflotran.org/documentation/user_guide/cards/process_model_cards/dataset_card.html?highlight=parameter#cell-indexed-datasets) (string)\n",
    "   - material_id_file: file path for the PFLOTRAN mesh file in HDF5 (string)\n",
    "   - obs_nc_original_file: file path for the original observation file in netCDF format (string)\n",
    "   - obs_nc_file: file path for the clipped observation file (only including the spatial and temporal datasets of interest) in netCDF format (string)\n",
    "   - other pflotran output and input file name and format options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PFLOTRAN parameter file and material id file\n",
    "files_cfg[\"pflotran_para_file\"] = os.path.join(pflotran_in_dir, \"parameter_prior_av_std1.5.h5\")\n",
    "files_cfg[\"material_id_file\"] = os.path.join(pflotran_in_dir, \"AT-mesh-alluvium-rivershape.h5\")\n",
    "\n",
    "# Observation file\n",
    "files_cfg[\"obs_nc_original_file\"] = os.path.join(pflotran_in_dir, \"SFA_300a_all_2017-2019.nc\")\n",
    "files_cfg[\"obs_nc_file\"] = os.path.join(pflotran_in_dir, \"SFA_300a_clipped.nc\")\n",
    "\n",
    "# PFLOTRAN output, where the state prior can be read from\n",
    "# dependending on the OUTPUT card, it can be from the observation file or from snapshot file\n",
    "files_cfg[\"pflotran_in_file\"] = os.path.join(pflotran_in_dir, \"AT-model-ensemble.in\")\n",
    "files_cfg[\"pflotran_out_prefix\"] = \"pflotran\"\n",
    "files_cfg[\"pflotran_out_file\"] = os.path.join(pflotran_out_dir, \"pflotranR[ENS].h5\")\n",
    "files_cfg[\"pflotran_restart_file\"] = os.path.join(pflotran_out_dir, \"pflotranR[ENS]-restart.h5\")\n",
    "files_cfg[\"pflotran_log_file\"] = os.path.join(pflotran_out_dir, \"pflotranR[ENS].out\")\n",
    "files_cfg[\"pflotran_obs_file\"] = os.path.join(pflotran_out_dir, \"pflotranR[ENS]-obs[ANY].tec\")\n",
    "files_cfg[\"use_obs_tecfile_for_prior\"] = True\n",
    "\n",
    "configs[\"other_dir_cfg\"] = dirs_cfg\n",
    "configs[\"file_cfg\" ]     = files_cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Specify the following types of variables used in DART**\n",
    "   - obs_set: the observation data to be assimilated (corresponding to the variable names in obs_nc_original_file) (list of string)\n",
    "   - obs_pflotran_set: the corresponding observation variable name in PFLOTRAN (list of string)\n",
    "   - para_set: the PFLOTRAN parameters to be analyzed (list of string)\n",
    "   - para_homogeneous: whether the parameters are homogeneous or heterogeneous (bool)\n",
    "   - para_material_id_set: the corresponding material IDs in PFLOTRAN (defined in both input deck and material_id_file) (list of integer)\n",
    "   - para_hdf_dataset_name_set: the DATASET name in pflotran_para_file (list of string)\n",
    "   - para_isotropic_set: whether the parameter is isotropic (list of bool)\n",
    "   - para_anisotropic_ratio_set: the anisotropic ratio between horizontal and vertical directions, less than one (list of float)\n",
    "   - para_resampled_set: the set of parameters whose prior would be sampled based on the posteor at the previous time step (list of string)\n",
    "   - para_sample_method_set: the sampling approach (list of string)\n",
    "   - the statistics of the parameters:  (1) the value range (i.e., para_min_set, para_max_set); (2) the mean and std for randomly sampling (i.e., para_mean_set, para_std_set).\n",
    "   - para_take_log_set: whether the parameters should be estimated based on its original value or log value (list of bool)\n",
    "   - use_default_para_initial: whether to use the default parameter ensemble as the initial before assimilation starts (bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation data to be assimilated\n",
    "# Note that the names should be identical to the variable names in the corresponding netCDF file\n",
    "obs_set  = ['WATER_LEVEL_SENSOR', 'NORMALIZED_SPC_SENSOR']\n",
    "\n",
    "# The corresponding observation variable in PFLOTRAN\n",
    "# Note that if it is WATER_LEVEL, it is converted from LIQUID_PRESSURE output from PFLOTRAN;\n",
    "#           if it is SPC, it is converted from the river tracer output from PFLOTRAN.\n",
    "obs_pflotran_set  = ['WATER_LEVEL', 'GROUNDWATER_TRACER']\n",
    "\n",
    "# The PFLOTRAN parameters to be analyzed\n",
    "# para_set = ['FLOW_FLUX','POROSITY','THERMAL_CONDUCTIVITY']\n",
    "para_set = ['HANFORD_PERMEABILITY', 'ALLUVIUM_PERMEABILITY']\n",
    "# para_set = ['PERMEABILITY']\n",
    "para_homogeneous = False\n",
    "if not para_homogeneous:\n",
    "    para_material_id_set  = [1, 5]  # the corresponding material IDs in PFLOTRAN; if multiple, then their sampling methods are different\n",
    "    para_hdf_dataset_name_set = ['perm', 'perm']   # the corresponding dataset name in parameter prior h5 file\n",
    "    para_isotropic_set = [False, True]  # the corresponding indication of whether they are isotropic\n",
    "    para_anisotropic_ratio_set = [1, 0.1]  # the corresponding indication of whether they are isotropic\n",
    "else:\n",
    "    para_material_id_set = [None, None]\n",
    "    para_hdf_dataset_name_set = para_set\n",
    "    para_isotropic_set = [False, False]\n",
    "    para_anisotropic_ratio_set = [1, 1]\n",
    "\n",
    "# Parameter sampling approach from posterior to prior\n",
    "para_resampled_set = ['HANFORD_PERMEABILITY', 'ALLUVIUM_PERMEABILITY']   # The parameters to be resampled at each time step\n",
    "# para_resampled_set = ['']   # The parameters to be resampled at each time step\n",
    "para_sample_method_set = [\"SGS\", \"SGS\"]  # the parameter sampling method, including: SGS, US, SIS, rescale, normal, lognormal, truncated_normal, uniform\n",
    "    \n",
    "# The statistics of the parameters\n",
    "# The index order follows para_set\n",
    "para_min_set  = [-12, -18]  # The minimum values (-99999 means no lower bound limit)\n",
    "para_max_set  = [-4, -4]  # The maximum values (99999 means no upper bound limit)\n",
    "para_mean_set = [None, None]  # The mean values\n",
    "para_std_set  = [None, None]  # The standard deviation values\n",
    "\n",
    "# Others\n",
    "para_take_log_set = [True, True] # Indicate whether the parameters should be estimated based on its original value or log value\n",
    "use_default_para_initial = True # Indicate whether using the default parameter ensemble as initial\n",
    "\n",
    "configs[\"obspara_set_cfg\"] = {\"obs_set\": obs_set, \"obs_pflotran_set\": obs_pflotran_set,\n",
    "                              \"para_set\": para_set, \"para_isotropic_set\": para_isotropic_set,\n",
    "                              \"para_material_id_set\": para_material_id_set, \"para_hdf_dataset_name_set\": para_hdf_dataset_name_set,\n",
    "                              \"para_take_log_set\": para_take_log_set, \"use_default_para_initial\":use_default_para_initial,\n",
    "                            #   \"para_dist_set\": para_dist_set, \"para_prior_rescaled_set\": para_prior_rescaled_set,\n",
    "                              \"para_homogeneous\": para_homogeneous, \"para_anisotropic_ratio_set\": para_anisotropic_ratio_set,\n",
    "                              \"para_min_set\": para_min_set, \"para_max_set\": para_max_set,\n",
    "                              \"para_mean_set\": para_mean_set, \"para_std_set\": para_std_set,\n",
    "                              \"para_sample_method_set\": para_sample_method_set, \"para_resampled_set\": para_resampled_set}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Specify the spatial domains of the observation data to be assimilated**\n",
    "\n",
    "   The limits of x, y, and z are bounded by the minimum and maximum boundaries through (min, max). If the limit is not specified, -99999 and 99999 are assigned for the lower and upper bounds, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space_xlimit = [-99999, 99999]\n",
    "obs_space_ylimit = [-99999, 99999]\n",
    "obs_space_zlimit = [-99999, 99999]\n",
    "configs[\"obs_space_cfg\"] = {\"obs_space_xlimit\": obs_space_xlimit, \"obs_space_ylimit\": obs_space_ylimit,\n",
    "                            \"obs_space_zlimit\": obs_space_zlimit}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Specify the data assimilation time window**\n",
    "   - assim_window_fixed: whether the assimilation window is fixed (bool)\n",
    "   - assim_window_days: the number of days for assimilation window if assim_window_fixed is true (integer)\n",
    "   - assim_window_seconds: the number of remaining seconds for assimilation window if assim_window_fixed is true (integer)\n",
    "   - assim_window_size: the assimilation window size (in fractional days) if assim_window_fixed is true (float)\n",
    "   - assim_window_list: the list of assimilation windows if assim_window_fixed is false (list of float)\n",
    "   - use_para_initial_at_nth_window: employ the intial parameter ensemble at the nth assimilation window; is negative if unused (integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_cfg = {}\n",
    "\n",
    "# Assimilation time window time_step_days+time_step_seconds\n",
    "# Assimilation window\n",
    "da_cfg[\"assim_window_fixed\"]   = True # whether the assimilation window is fixed (day)\n",
    "da_cfg[\"assim_window_days\"]    = 0     # assimilation time window/step (day)\n",
    "da_cfg[\"assim_window_seconds\"] = 15*24*3600  # assimilation time window/step  (second)\n",
    "# da_cfg[\"assim_window_seconds\"] = 30*24*3600  # assimilation time window/step  (second)\n",
    "# da_cfg[\"assim_window_seconds\"] = 24*3600  # assimilation time window/step  (second)\n",
    "da_cfg[\"assim_window_size\"] = da_cfg[\"assim_window_days\"]+float(da_cfg[\"assim_window_seconds\"])/86400. # day\n",
    "first_assim_window_size = da_cfg[\"assim_window_size\"]\n",
    "# da_cfg[\"assim_window_list\"] = [2., (4000.*60+10)/86400.] # day\n",
    "da_cfg[\"use_para_initial_at_nth_window\"] = -1 # Use the initial parameter ensemble at the nth assimilation window (unused if negative)\n",
    "# first_assim_window_size = iada_cfg[\"assim_window_list\"][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Specify the temporal information**\n",
    "   - spinup_length_days: the number of days for model spinup (integer)\n",
    "   - spinup_length_seconds: the remaining seconds for model spinup (integer)\n",
    "   - spinup_length: the overall spinup time in fractional days (float)\n",
    "   - is_spinup_done: whether the model spinup is conducted (bool)\n",
    "   - current_model_time: the current model time and the first one would be the sum of spinup_length and harf of the first assimilation window size, in fractional days (float)\n",
    "   - model_time_list: the list of the model time (list of float)\n",
    "   - model_start and assim_start: the start of assimilation time in calendar time (string)\n",
    "   - first_obs_time_days: the time of the first observation to be assimilated in day (integer)\n",
    "   - first_obs_time_seconds: the time of the first observation to be assimilated in second (integer)\n",
    "   - first_obs_time: the time of the first observation to be assimilated in fractional day (float)\n",
    "   - last_obs_time_days: the time of the last observation to be assimilated in day (integer)\n",
    "   - last_obs_time_seconds: the time of the last observation to be assimilated in second (integer)\n",
    "   - last_obs_time: the time of the last observation to be assimilated in fractional day (float)\n",
    "   - exceeds_obs_time: whether the current model time exceeds the last observation time, indicating whether the assimilation is done (bool)\n",
    "\n",
    "**note that** model start time is considered after the spinup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cfg = {}\n",
    "one_sec = 1./86400. # fraction of day\n",
    "# Model spinup length\n",
    "time_cfg[\"spinup_length_days\"] = 360    # number of days for spinup\n",
    "time_cfg[\"spinup_length_seconds\"] = 0    # number of seconds for the remaining spinup\n",
    "time_cfg[\"spinup_length\"]  = time_cfg[\"spinup_length_days\"]+float(time_cfg[\"spinup_length_seconds\"])/86400. # spinup time (day)\n",
    "time_cfg[\"is_spinup_done\"] = True  # whether spinup is conducted\n",
    "\n",
    "# Model start time\n",
    "time_cfg[\"current_model_time\"] = time_cfg[\"spinup_length\"] + (first_assim_window_size + one_sec)/2  # model start time should be the middle of the first assimilation window (after spinup)\n",
    "time_cfg[\"model_time_list\"]    = [time_cfg[\"current_model_time\"]]   # the list of model time\n",
    "\n",
    "# Map between observation assimilation start time and model start time\n",
    "obs_start   = datetime(2017,1,1,0,0,0) + timedelta(days=time_cfg[\"spinup_length_days\"], seconds=time_cfg[\"spinup_length_seconds\"])\n",
    "# assim_start = obs_start + timedelta(days=time_cfg[\"spinup_length\"]) # assimilation time should be after the model spinup\n",
    "assim_start = obs_start # assimilation time should be after the model spinup\n",
    "time_cfg[\"model_start\"] = obs_start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "time_cfg[\"assim_start\"] = assim_start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# The maximum time for observation\n",
    "time_cfg[\"first_obs_time_days\"]    = time_cfg[\"spinup_length_days\"]\n",
    "time_cfg[\"first_obs_time_seconds\"] = time_cfg[\"spinup_length_seconds\"]\n",
    "time_cfg[\"first_obs_time_size\"] = time_cfg[\"first_obs_time_days\"]+float(time_cfg[\"first_obs_time_seconds\"])/86400. # day\n",
    "if da_cfg[\"assim_window_fixed\"]:\n",
    "    time_cfg[\"last_obs_time_days\"]    = time_cfg[\"first_obs_time_days\"]\n",
    "    time_cfg[\"last_obs_time_seconds\"] = time_cfg[\"first_obs_time_seconds\"] + 2*da_cfg[\"assim_window_seconds\"]\n",
    "    # time_cfg[\"last_obs_time_seconds\"] = time_cfg[\"first_obs_time_seconds\"] + 3600*24*31\n",
    "else:\n",
    "    total_time = np.sum(da_cfg[\"assim_window_list\"])\n",
    "    time_cfg[\"last_obs_time_days\"] = time_cfg[\"first_obs_time_days\"] + floor(total_time)\n",
    "    time_cfg[\"last_obs_time_seconds\"] = time_cfg[\"first_obs_time_seconds\"] + int((total_time-floor(total_time)*86400))\n",
    "time_cfg[\"last_obs_time_size\"] = time_cfg[\"last_obs_time_days\"]+float(time_cfg[\"last_obs_time_seconds\"])/86400. # day\n",
    "\n",
    "# Whether the model time exceeds the last observation\n",
    "time_cfg[\"exceeds_obs_time\"] = time_cfg[\"current_model_time\"] >= time_cfg[\"last_obs_time_size\"]\n",
    "\n",
    "# Save them to configs\n",
    "configs[\"time_cfg\"] = time_cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Config the data assimilation**\n",
    "   - nens: number of ensemble members (integer)\n",
    "   - obs_error: the error of all observation variables in obs_set (list of float)\n",
    "   - obs_error_type: the type of observation error (i.e., 'relative' and 'absolute') (string)\n",
    "   - assim_start_days: the start time of the current assimilation window in day (integer)\n",
    "   - assim_start_seconds: the remaining seconds of the start time of the current assimilation window (integer)\n",
    "   - assim_end_days: the end time of the current assimilation window in day (integer)\n",
    "   - assim_end_seconds: the remaining seconds of the end time of the current assimilation window (integer)\n",
    "   - ntimestep: the number of time steps or assimilation windows (integer)\n",
    "   - obs_nes_posterior_from_model: whether the observation posterior should be recomputed from the model output based on the parameter posterior (bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation error, number of ensembles\n",
    "da_cfg[\"nens\"]      = 100        # number of ensembles\n",
    "da_cfg[\"obs_error\"] = [0.001, 0.05]       # the observation error (with the order corresponding to the obs_set)\n",
    "da_cfg[\"obs_error_type\"] = [\"relative\", \"relative\"] # the type of observation error (i.e., relative and absolute)\n",
    "\n",
    "# The start and end time of the current assimilation time window\n",
    "da_cfg[\"assim_start_days\"]    = int(floor(time_cfg[\"current_model_time\"] - first_assim_window_size/2))\n",
    "da_cfg[\"assim_start_seconds\"] = int((time_cfg[\"current_model_time\"] - first_assim_window_size/2 - da_cfg[\"assim_start_days\"])*86400+1)\n",
    "da_cfg[\"assim_end_days\"]      = int(floor(time_cfg[\"current_model_time\"] + first_assim_window_size/2))\n",
    "da_cfg[\"assim_end_seconds\"]   = int((time_cfg[\"current_model_time\"] + first_assim_window_size/2 - da_cfg[\"assim_end_days\"])*86400)\n",
    "\n",
    "# Compute the number of time steps\n",
    "# print(np.ceil((time_cfg[\"last_obs_time_size\"] - time_cfg[\"first_obs_time_size\"]) / da_cfg[\"assim_window_size\"]))\n",
    "if da_cfg[\"assim_window_fixed\"]:\n",
    "    da_cfg[\"ntimestep\"] = ceil((time_cfg[\"last_obs_time_size\"] - time_cfg[\"first_obs_time_size\"]) / da_cfg[\"assim_window_size\"])\n",
    "else:\n",
    "    da_cfg[\"ntimestep\"] = len(da_cfg[\"assim_window_list\"])\n",
    "\n",
    "# Decide whether the observation posterior should be recomputed from the model output based on the parameter posterior\n",
    "da_cfg[\"obs_ens_posterior_from_model\"] = True\n",
    "\n",
    "# TODO: remove this once this issue is fixed in PFLOTRAN\n",
    "# Check whether the number of realizations are divisible by the number of cores used for running PFLOTRAN\n",
    "if ncore_pf % da_cfg[\"nens\"] != 0:\n",
    "    raise Exception(\"the number of realizations {} are divisible by the number of cores {} used for running PFLOTRAN\".format(da_cfg[\"nens\"],ncore_pf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Config the MDA settings**\n",
    "   - enks_mda_alpha: the list of MDA coefficient with the summation of their inverse equal to one (list of float)\n",
    "   - enks_mda_iteration_step: the current iteration step, staring from one (integer)\n",
    "   - enks_mda_total_iterations: the total iterations (integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inflation settings used in EnKS-MDA (the alpha value)\n",
    "da_cfg[\"enks_mda_alpha\"] = [4., 4., 4., 4.]  # Note that the summation of the inverse of alpha should be one\n",
    "# da_cfg[\"enks_mda_alpha\"] = [3., 3., 3.]  # Note that the summation of the inverse of alpha should be one\n",
    "# da_cfg[\"enks_mda_alpha\"] = [2., 2.]  # Note that the summation of the inverse of alpha should be one\n",
    "# da_cfg[\"enks_mda_alpha\"] = [1.]  # Note that the summation of the inverse of alpha should be one\n",
    "da_cfg[\"enks_mda_iteration_step\"] = 1  # the ith iteration (1 for the first iteration)\n",
    "da_cfg[\"enks_mda_total_iterations\"] = len(da_cfg[\"enks_mda_alpha\"])  # Note that the summation of the inverse of alpha should be one\n",
    "\n",
    "# Check whether the sum of the inverse of enks_mda_alpha is one\n",
    "alpha_inv_sum = sum([1./alpha for alpha in da_cfg[\"enks_mda_alpha\"]])\n",
    "if alpha_inv_sum - 1 > 1e-8:\n",
    "    raise Exception(\"The sum of the inverse of enks_mda_alpha should be one!\")\n",
    "\n",
    "# Save them to configs\n",
    "configs[\"da_cfg\"] = da_cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Save all the configurations in ```config_file```.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.write(config_file, force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Step 2: PFLOTRAN preparation\n",
    "<a id='pflotran_prepare'></a>\n",
    "\n",
    "   In this section, we (1) generate the parameter files in HDF 5, ```parameter_prior.h5```, used by PFLOTRAN input deck file; and (2) perform model spin-up if required.\n",
    "\n",
    "**Note that**\n",
    "   - The PFLOTRAN input deck ```files_cfg[\"pflotran_in_file\"]``` should be prepared by users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_pflotran_inputdeck, pflotran_in = files_cfg[\"prep_pflotran_inputdeck_file\"], files_cfg[\"pflotran_in_file\"]\n",
    "prep_pflotran_parameterprior = files_cfg[\"prep_pflotran_para_file\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Generate the ensembles of PFLOTRAN prior**\n",
    "\n",
    "   **Run code**\n",
    "   - Run: ```prepare_pflotran_parameterprior.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$prep_pflotran_parameterprior\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Prepare PFLOTRAN ensemble parameter values...\")\n",
    "subprocess.run(\"python {} {}\".format(prep_pflotran_parameterprior, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "****************\n",
    "**Conduct the model spin-up**\n",
    "   \n",
    "Take in the ```files_cfg[\"pflotran_in_file\"]``` and ```files_cfg[\"pflotran_para_file\"]``` files and conduct the model spin-up by running ```pflotran.sh``` file. The ```pflotran.sh``` is a shell script executing ensemble simulation of PFLOTRAN by using MPI.\n",
    "\n",
    "   **Run the code**\n",
    "   - Run: ```run_pflotran.py```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pflotran_sh, pflotran_out_dir = files_cfg[\"pflotran_sh_file\"], dirs_cfg[\"pflotran_out_dir\"]\n",
    "run_pflotran, pflotran_out_dir = files_cfg[\"run_pflotran_file\"], dirs_cfg[\"pflotran_out_dir\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Model spinup...\")\n",
    "if time_cfg['spinup_length'] != 0 and not configs[\"time_cfg\"][\"is_spinup_done\"]:\n",
    "    subprocess.run(\"python {} {}\".format(run_pflotran, config_file), shell=True, check=True)\n",
    "else:\n",
    "    print(\"Model spinup is already done or not required.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ****************\n",
    "   **Once the model spinup finishes, modify the corresponding configuration entry.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs[\"time_cfg\"][\"is_spinup_done\"] = True\n",
    "configs.write(config_file, force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Step 3: DART preparation\n",
    "<a id='dart_prepare'></a>\n",
    "   In this section, the following procedures are performed:\n",
    "   - generate the template for DART generic variable quantity files (i.e., ```DEFAULT_obs_kind_mod.F90``` and ```obs_def_pflotran_mod.f90```);\n",
    "   - generate the DART input namelists;\n",
    "   - check whether the observation netCDF file exists;\n",
    "   - prepare the ```convert_nc.f90``` based on the list of observation variables;\n",
    "   - generate all the executables;\n",
    "   - check ```model_mod.F90``` based on current setting by using the ```check_model_mod``` provided by DART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************\n",
    "**Generate the templates for DART generic variable quantity files**\n",
    "- Run: ```list2dartqty.py``` to sequentially generate/modify\n",
    "    - a mapping between PFLOTRAN variales and DART generic quantities in ```obs_def_pflotran_mod.F90```\n",
    "    - the default DART generic quantity definition file ```DEFAULT_obs_kind_mod.F90```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dartqty, obs_type_file = files_cfg[\"to_dartqty_file\"], files_cfg[\"obs_type_file\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$to_dartqty\" \"$config_file\"', 'python $1 $2 $3 $4')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Add PFLOTRAN variables to DEFAULT_obs_kind_mod.F90 if necessary...\")\n",
    "subprocess.run(\"python {} {}\".format(to_dartqty, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************\n",
    "**Generate  DART input namelists in ```input.nml```**\n",
    "   The ```input.nml``` file is generated based on a template ```input.nml.template``` by modifying the following namelist entries:\n",
    "\n",
    "   ```input.nml.template``` $\\rightarrow$ ```input.nml```\n",
    "\n",
    "**Namelists from DART**\n",
    "   - [obs_kind_nml](https://www.image.ucar.edu/DAReS/DART/Manhattan/assimilation_code/modules/observations/obs_kind_mod.html#Namelist): namelist for controling what observation types are to be assimilated\n",
    "   - [preprocess_nml](https://www.image.ucar.edu/DAReS/Codes/DART/manhattan/assimilation_code/programs/preprocess/preprocess): namelist of the DART-supplied preprocessor program which creates observation kind and observation definition modules from a set of other specially formatted Fortran 90 files\n",
    "   - and others (see DART documentation for details.)\n",
    "\n",
    "**Self-defined namelists**\n",
    "   - smoother_nml: a self-defined namelist of the main module for driving ensemble data assimilations modified from DART [filter_nml](https://www.image.ucar.edu/DAReS/DART/Manhattan/assimilation_code/modules/assimilation/filter_mod.html#Namelist)\n",
    "   - model_nml: a self-defined namelist for providing the basic information in the model\n",
    "       - template_file: the template prior NetCDF file for ```model_mod.F90``` to digest the spatial information of the model (string)\n",
    "       - nvar_para: the number of parameter variables (integer)\n",
    "       - para_var_names: the original variable names for model parameters (list of string)\n",
    "       - para_var_qtynames: the corresponding DART variable quantities for model parameters (list of string)\n",
    "       - nvar_state: the number of state variables (integer)\n",
    "       - state_var_names: the original variable names for model states (list of string)\n",
    "       - state_var_qtynames: the corresponding DART variable quantities for model states (list of string)\n",
    "       - debug: whether the debug mode is turned on (bool)\n",
    "       - max_time_diff_seconds: the maximum time difference allowed for finding the closest ensemble for observations in second (integer)\n",
    "\n",
    "   - convertnc_nml: a self-defined namelist for providing the NetCDF observation file name and the DART observation file name used in ```convert_nc.f90```\n",
    "       - netcdf_file: the location of the NetCDF file containing the observation data (string)\n",
    "       - out_file: the location of the DART observation file (string)\n",
    "\n",
    "**Note that**\n",
    "   - There are more namelists or other items in the above namelist in input.nml.template. Users can edit the below python dictionary ```inputnml``` to include their modifications.\n",
    "   - Users can also include more namelists provided by DART by modifying ```input.nml```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************\n",
    "**Assemble all the namelists in input.nml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for different namelists in input.nml\n",
    "smoother_nml = {\"input_state_file_list\":files_cfg[\"dart_input_list_file\"],\n",
    "              \"output_state_file_list\":files_cfg[\"dart_output_list_file\"],\n",
    "              \"ens_size\":da_cfg[\"nens\"],\n",
    "              \"num_output_state_members\":da_cfg[\"nens\"],\n",
    "              \"obs_sequence_in_name\":files_cfg[\"obs_dart_file\"],\n",
    "              \"first_obs_days\": da_cfg[\"assim_start_days\"],\n",
    "              \"first_obs_seconds\": da_cfg[\"assim_start_seconds\"],\n",
    "              \"last_obs_days\": da_cfg[\"assim_end_days\"],\n",
    "              \"last_obs_seconds\": da_cfg[\"assim_end_seconds\"],\n",
    "              \"output_mean\": False,\n",
    "              \"output_sd\": False}\n",
    "obs_kind_nml = {\"assimilate_these_obs_types\":obs_set}\n",
    "assim_tools_nml = {\"filter_kind\":2, \"sort_obs_inc\": True}\n",
    "model_nml = {\"nvar_state\":len(obs_pflotran_set),\n",
    "             \"state_var_names\":obs_pflotran_set,\n",
    "             \"state_var_qtynames\":['QTY_PFLOTRAN_'+v for v in obs_pflotran_set],\n",
    "             \"nvar_para\":len(para_set),\n",
    "             \"para_var_names\":para_set,\n",
    "             \"para_var_qtynames\":['QTY_PFLOTRAN_'+v for v in para_set],\n",
    "             \"max_time_diff_seconds\": 10,\n",
    "             \"debug\": False,\n",
    "             \"template_file\":files_cfg[\"dart_prior_template_file\"]}\n",
    "preprocess_nml = {\"input_files\":files_cfg[\"obs_type_file\"],\n",
    "                  \"input_obs_kind_mod_file\":files_cfg[\"def_obs_kind_file\"]}\n",
    "convertnc_nml = {\"netcdf_file\": files_cfg[\"obs_nc_file\"],\n",
    "                 \"out_file\": files_cfg[\"obs_dart_file\"],\n",
    "                 \"obs_start_day\": da_cfg[\"assim_start_days\"],\n",
    "                 \"obs_start_second\": da_cfg[\"assim_start_seconds\"],\n",
    "                 \"obs_end_day\": da_cfg[\"assim_end_days\"],\n",
    "                 \"obs_end_second\":da_cfg[\"assim_end_seconds\"],\n",
    "                 \"inflation_alpha\":da_cfg[\"enks_mda_alpha\"][da_cfg[\"enks_mda_iteration_step\"]-1]}\n",
    "modelmodcheck_nml = {\"input_state_files\": files_cfg[\"dart_prior_template_file\"]}\n",
    "inputnml = {\"smoother_nml\":smoother_nml,\n",
    "            \"obs_kind_nml\":obs_kind_nml,\n",
    "            \"assim_tools_nml\":assim_tools_nml,\n",
    "            \"model_nml\":model_nml,\n",
    "            \"preprocess_nml\":preprocess_nml,\n",
    "            \"convert_nc_nml\":convertnc_nml,\n",
    "            \"model_mod_check_nml\":modelmodcheck_nml}\n",
    "\n",
    "\n",
    "configs[\"inputnml_cfg\"] = inputnml\n",
    "\n",
    "# Save the configurations\n",
    "configs.write(config_file, force=True)\n",
    "# with open(config_pickle, 'wb') as f:\n",
    "#     pickle.dump(configs, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ***************\n",
    "   **Run the code**\n",
    "   - Run: ```prepare_inputnml.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_inputnml = files_cfg[\"prep_inputnml_file\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s  \"$prep_inputnml\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Generate input.nml file...\")\n",
    "subprocess.run(\"python {} {}\".format(prep_inputnml, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************\n",
    "**Prepare the observation conversion to DART observation format**\n",
    "\n",
    "In this section, we prepare the process of converting the observation data to DART format. We first convert observation data in raw format into NetCDF format. Then, a fortran script is prepared for the conversion from the NetCDF to to DART format. The structure of NetCDF file for recording observation file.\n",
    "\n",
    "   | NetCDF dimensions |           NetCDF variables          |\n",
    "   |:-----------------:|:-----------------------------------:|\n",
    "   | time: ntime       | time: shape(time)                   |\n",
    "   | location: nloc    | location: shape(location)           |\n",
    "   |                   | each observation: shape(ntime,nloc) |\n",
    "   |                   | each observation error: shape(ntime,nloc) |\n",
    "\n",
    "   **Note that**\n",
    "   - if the time calendar follows *gregorian*, the time unit should be entered as ```seconds since YYYY-MM-DD HH:MM:SS```. Otherwise, put the time calender as *None* and time unit as ```second``` (make sure convert your measurement times to seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************\n",
    "**Check whether observation data in netCDF exists in files_cfg[\"obs_nc_original_file\"]``````**\n",
    "\n",
    "**Note that**\n",
    "- The conversion to netCDF format can be arbitrary based on the raw data format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_nc, obs_nc_original = files_cfg[\"csv_to_nc_file\"], files_cfg[\"obs_nc_original_file\"]\n",
    "if os.path.isfile(obs_nc_original):\n",
    "    print(\"\\n\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(\"Observation file is found.\")\n",
    "else:\n",
    "    print(\"\\n\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    raise Exception(\"Convertion raw observation data to NetCDF file is required...\")\n",
    "    # print(\"Convert raw observation data to NetCDF file...\")\n",
    "    # subprocess.run(\"python {} {}\".format(csv_to_nc, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ***************\n",
    "   **Clip the NetCDF file based on the defined spatial and temporal domains**\n",
    "\n",
    "   The NetCDF file generated in the previous step is further processed by selecting data observed in the required spatial and temporal domains\n",
    "   - Run: ```clip_obs_nc.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Clip the NetCDF file based on the defined spatial and temporal domains...\")\n",
    "clip_obs_nc, obs_nc = files_cfg[\"clip_obs_nc_file\"], files_cfg[\"obs_nc_file\"]\n",
    "subprocess.run(\"python {} {}\".format(clip_obs_nc, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ***************\n",
    "   **Prepare the ```convert_nc.f90``` based on the list of observation variables**\n",
    "   - Run: ```prepare_convert_nc.py```\n",
    "   - Code input arguments:\n",
    "       - obs_nc: filename for the observation NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_convert_nc, convert_nc_file = files_cfg[\"prep_convert_nc_file\"], files_cfg[\"convert_nc_file\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_cell_magic('script', 'bash -s \"$prep_convert_nc\" \"$config_file\"', 'python $1 $2')\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Prepare the convert_nc.f90 based on the list of observation variables to be assimilated...\")\n",
    "subprocess.run(\"python {} {}\".format(prep_convert_nc, config_file), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "****************\n",
    "**Generate all the executable files**\n",
    "\n",
    "Now, we compile all the executables from ```mkmf_*```. The following executables are generated here:\n",
    "   - ```preprocess```: for preprocessing the [prepared DART generic variable quantity files prepared](#dart_generic_prepare)\n",
    "   - ```convert_nc```: for [converting the observations from NetCDF to DART format](#observationconvertion)\n",
    "   - ```model_mod_check```: for checking ```model_mod.F90``` interface file\n",
    "   - ```smoother```: for conducting the DART data assimilation\n",
    "\n",
    "Run the code\n",
    "   - Run: ```quickbuild.csh```\n",
    "   - Code input arguments:\n",
    "       - app_work_dir: location of the application work folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dart_work_dir, app_work_dir = dirs_cfg[\"dart_work_dir\"], dirs_cfg[\"app_work_dir\"]\n",
    "quickbuild = files_cfg[\"quickbuild_csh\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Generate all the executables...\")\n",
    "# subprocess.run(\"cd {}; csh {} {} -mpi\".format(dart_work_dir, quickbuild, app_work_dir), shell=True, check=True)\n",
    "# subprocess.run(\"cd {}; csh {} {}\".format(dart_work_dir, quickbuild, app_work_dir), shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************\n",
    "**Check ```model_mod.F90``` interface file**\n",
    "- Run: ```model_mod_check```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mod_check = files_cfg[\"model_mod_check_exe\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Perform the sequential ES-MDA\n",
    "<a id='run_dart_pflotran'></a>\n",
    "   In this section, run the shell script to perform the sequential ES-MDA using DART-PFLOTRAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dart_work_dir     = dirs_cfg[\"dart_work_dir\"]\n",
    "run_DART_PFLOTRAN = files_cfg[\"run_da_csh\"]\n",
    "concatenate_output = files_cfg[\"concatenate_dart_output_file\"]\n",
    "inputnml_file     = files_cfg[\"input_nml_file\"]\n",
    "start_time        = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************\n",
    "**[Option 1] Run the C shell script directly here (for small applications)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Assimilation starts here...\")\n",
    "subprocess.run(\"cd {}; csh {} {} {} | tee {}\".format(dart_work_dir, run_DART_PFLOTRAN, \n",
    "                                                     inputnml_file, config_file, log_file), shell=True, check=True)\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(\"The total time usage of running DART and PFLOTRAN is %.3f (second): \" % (end_time-start_time))\n",
    "exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************\n",
    "**[Option 2] Submit the job using nohup (for large applications)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "print(\"\\n\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Assimilation starts here...\")\n",
    "subprocess.run(\"cd {}; nohup csh {} {} {} > {} &\".format(dart_work_dir, run_DART_PFLOTRAN, inputnml_file, \n",
    "                                                  config_file, log_file), shell=True, check=True)\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(\"The total time usage of running DART and PFLOTRAN is %.3f (second): \" % (end_time-start_time))\n",
    "# exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# [Optional] Submit the job to Cori\n",
    "**Important:** if you want to submit the job, do not run the above cells, instead, (1) configure all the above settings, and (2) run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_file = 'workflow.ipynb'\n",
    "script_file   = 'workflow.py'\n",
    "sbatch_file   = 'sbatch_da.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************\n",
    "**Convert the jupyter notebook to python script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook workflow.ipynb to script\n",
      "[NbConvertApp] Writing 38795 bytes to workflow.py\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to script $notebook_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************\n",
    "**Generate the sbatch file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sbatch_file, 'w') as f:\n",
    "    f.write(\"#!/bin/bash \\n\")\n",
    "    f.write(\"#SBATCH -A m1800 \\n\")\n",
    "    f.write(\"#SBATCH -q regular \\n\")\n",
    "    f.write(\"#SBATCH -N 96 \\n\")\n",
    "    f.write(\"#SBATCH -t 48:00:00 \\n\")\n",
    "    f.write(\"#SBATCH -L SCRATCH \\n\")\n",
    "    f.write(\"#SBATCH -C haswell \\n\")\n",
    "    f.write(\"#SBATCH -J DTPF_300A \\n\")\n",
    "    f.write(\"#SBATCH --mail-type=begin,end,fail \\n\")\n",
    "    f.write(\"#SBATCH --mail-user=peishi.jiang@pnnl.gov \\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"python {} \\n\".format(script_file))\n",
    "    f.write(\"wait\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************\n",
    "**Submit the sbatch file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! sbatch !sbatch_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
